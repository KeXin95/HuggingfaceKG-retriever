{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2dfe759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(\"NumPy version:\", numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6307d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hice1/cxu371/scratch/miniconda3/envs/mlg_fix/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0521add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755c97c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/pip\r\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7402e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/cxu371/scratch/miniconda3/envs/mlg_fix/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.llm import RAGQueryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296f3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.llm.models import (\n",
    "    LLM,\n",
    "    TXT2KG,\n",
    "    GRetriever,\n",
    "    LLMJudge,\n",
    "    SentenceTransformer,\n",
    ")\n",
    "from torch_geometric.llm.models.txt2kg import _chunk_text\n",
    "from torch_geometric.llm.utils.backend_utils import (\n",
    "    create_graph_from_triples,\n",
    "    create_remote_backend_from_graph_data,\n",
    "    make_pcst_filter,\n",
    "    preprocess_triplet,\n",
    ")\n",
    "from torch_geometric.llm.utils.feature_store import KNNRAGFeatureStore\n",
    "from torch_geometric.llm.utils.graph_store import NeighborSamplingRAGGraphStore\n",
    "from torch_geometric.llm.utils.vectorrag import DocumentRetriever\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GAT, SGFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834a9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ---- Load the graph ----\n",
    "# graph_path = \"./final_graph.pt\"\n",
    "# print(f\"üîç Loading graph from: {graph_path}\")\n",
    "# graph = torch.load(graph_path, weights_only=False)  # keeps full Data object\n",
    "# print(\"‚úÖ Graph loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0313c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRAPH SUMMARY ===\n",
      "Graph(x=[107694, 822], y=[107694, 54], train_mask=[107694], val_mask=[107694], test_mask=[107694], edge_index=[2, 124056], edge_attr=[124056])\n",
      "Num nodes: 107694\n",
      "Num edges: 124056\n",
      "Node feature dim: 822\n",
      "Label dim: 54\n",
      "train_mask: 97539 nodes (90.57%)\n",
      "val_mask: 7953 nodes (7.38%)\n",
      "test_mask: 659 nodes (0.61%)\n",
      "\n",
      "=== Example Edges (src ‚Üí dst) ===\n",
      "0 ‚Üí 34\n",
      "0 ‚Üí 57\n",
      "0 ‚Üí 222\n",
      "1 ‚Üí 258\n",
      "1 ‚Üí 47\n",
      "1 ‚Üí 324\n",
      "1 ‚Üí 723\n",
      "1 ‚Üí 724\n",
      "1 ‚Üí 2151\n",
      "1 ‚Üí 2182\n",
      "\n",
      "=== Edge Attributes (first 3) ===\n",
      "tensor([0, 3, 3])\n",
      "Edge attr shape: torch.Size([124056])\n",
      "\n",
      "=== Node Features (first 3) ===\n",
      "tensor([[ 0.0458,  0.0293, -0.0242,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0130, -0.0175,  0.0066,  ...,  0.0000,  0.9181,  0.0000],\n",
      "        [-0.0117, -0.0524,  0.0065,  ...,  0.0000,  0.8182,  0.0000]])\n",
      "Feature tensor shape: torch.Size([107694, 822])\n",
      "\n",
      "=== Node Labels (first 3, nonzero label indices) ===\n",
      "Node 0 labels ‚Üí [19]\n",
      "Node 1 labels ‚Üí [0]\n",
      "Node 2 labels ‚Üí [0]\n",
      "\n",
      "=== Structural Stats ===\n",
      "Average node degree: 1.15\n",
      "Max node degree: 5561\n",
      "Graph density: 0.00001070\n",
      "Approx. feature matrix size: 337.69 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=== GRAPH SUMMARY ===\")\n",
    "print(graph)\n",
    "print(f\"Num nodes: {graph.num_nodes}\")\n",
    "print(f\"Num edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dim: {graph.x.size(1) if hasattr(graph, 'x') else 'N/A'}\")\n",
    "print(f\"Label dim: {graph.y.size(1) if hasattr(graph, 'y') else 'N/A'}\")\n",
    "\n",
    "# ---- Split masks ----\n",
    "for mask_name in [\"train_mask\", \"val_mask\", \"test_mask\"]:\n",
    "    if hasattr(graph, mask_name):\n",
    "        mask = getattr(graph, mask_name)\n",
    "        print(f\"{mask_name}: {mask.sum().item()} nodes ({mask.sum().item()/graph.num_nodes:.2%})\")\n",
    "\n",
    "# ---- Edge inspection ----\n",
    "print(\"\\n=== Example Edges (src ‚Üí dst) ===\")\n",
    "edge_index = graph.edge_index\n",
    "if isinstance(edge_index, tuple) and len(edge_index) == 2:\n",
    "    src, dst = edge_index\n",
    "else:\n",
    "    src, dst = edge_index[0], edge_index[1]\n",
    "for i in range(min(10, src.size(0))):\n",
    "    print(f\"{src[i].item()} ‚Üí {dst[i].item()}\")\n",
    "\n",
    "# ---- Edge attributes ----\n",
    "if hasattr(graph, \"edge_attr\") and graph.edge_attr is not None:\n",
    "    print(\"\\n=== Edge Attributes (first 3) ===\")\n",
    "    print(graph.edge_attr[:3])\n",
    "    print(f\"Edge attr shape: {graph.edge_attr.shape}\")\n",
    "\n",
    "# ---- Node features ----\n",
    "if hasattr(graph, \"x\"):\n",
    "    print(\"\\n=== Node Features (first 3) ===\")\n",
    "    print(graph.x[:3])\n",
    "    print(f\"Feature tensor shape: {graph.x.shape}\")\n",
    "\n",
    "# ---- Node labels ----\n",
    "if hasattr(graph, \"y\"):\n",
    "    print(\"\\n=== Node Labels (first 3, nonzero label indices) ===\")\n",
    "    for i in range(3):\n",
    "        label_indices = torch.nonzero(graph.y[i]).flatten().tolist()\n",
    "        print(f\"Node {i} labels ‚Üí {label_indices}\")\n",
    "\n",
    "# ---- Node IDs (if exist) ----\n",
    "if hasattr(graph, \"node_id\"):\n",
    "    print(\"\\n=== Node IDs (first 10) ===\")\n",
    "    print(graph.node_id[:10])\n",
    "\n",
    "# ---- Basic graph stats ----\n",
    "print(\"\\n=== Structural Stats ===\")\n",
    "degrees = torch.bincount(graph.edge_index[0], minlength=graph.num_nodes)\n",
    "print(f\"Average node degree: {degrees.float().mean():.2f}\")\n",
    "print(f\"Max node degree: {degrees.max().item()}\")\n",
    "print(f\"Graph density: {graph.num_edges / (graph.num_nodes ** 2):.8f}\")\n",
    "\n",
    "# ---- Memory footprint ----\n",
    "if hasattr(graph, \"x\"):\n",
    "    feature_mem_mb = graph.x.numel() * graph.x.element_size() / 1024**2\n",
    "    print(f\"Approx. feature matrix size: {feature_mem_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1374cf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Available memory before load: 170.2 GB\n",
      "üìÇ Loading ./nodes_df.pkl ... this may take a few minutes if large.\n",
      "\n",
      "‚úÖ Loaded successfully!\n",
      "Shape: (389575, 7)\n",
      "Memory usage: 3.08 GB\n",
      "\n",
      "=== Columns ===\n",
      "['id', 'description', 'createdAt', 'type', 'y_multi_lab', 'relationships', 'y']\n",
      "\n",
      "=== Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 389575 entries, 0 to 389574\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             389575 non-null  object\n",
      " 1   description    389575 non-null  object\n",
      " 2   createdAt      389575 non-null  object\n",
      " 3   type           389575 non-null  object\n",
      " 4   y_multi_lab    389575 non-null  object\n",
      " 5   relationships  389575 non-null  object\n",
      " 6   y              389575 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 3.1 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 0)\n",
    "\n",
    "# --- Check available memory ---\n",
    "mem_gb = psutil.virtual_memory().available / 1024**3\n",
    "print(f\"üß† Available memory before load: {mem_gb:.1f} GB\")\n",
    "\n",
    "# --- Load the pickle ---\n",
    "path = \"./nodes_df.pkl\"\n",
    "print(f\"üìÇ Loading {path} ... this may take a few minutes if large.\")\n",
    "nodes_df = pd.read_pickle(path)\n",
    "\n",
    "print(\"\\n‚úÖ Loaded successfully!\")\n",
    "print(f\"Shape: {nodes_df.shape}\")\n",
    "print(f\"Memory usage: {nodes_df.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "\n",
    "# --- Inspect structure ---\n",
    "print(\"\\n=== Columns ===\")\n",
    "print(nodes_df.columns.tolist())\n",
    "\n",
    "print(\"\\n=== Info ===\")\n",
    "print(nodes_df.info(memory_usage='deep'))\n",
    "\n",
    "# # --- Show first few rows ---\n",
    "# print(\"\\n=== Sample Rows (first 5) ===\")\n",
    "# display(nodes_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed37a3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>type</th>\n",
       "      <th>y_multi_lab</th>\n",
       "      <th>relationships</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>model</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>model</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen/Qwen3-Next-80B-A3B-Instruct</td>\n",
       "      <td>---\\nlibrary_name: transformers\\nlicense: apache-2.0\\nlicense_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE\\npipeline_tag: text-generation\\n---\\n\\n# Qwen3-Next-80B-A3B-Instruct\\n&lt;a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n&lt;/a&gt;\\n\\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). \\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. \\nWe call this next-generation foundation models **Qwen3-Next**.\\n\\n## Highlights\\n\\n**Qwen3-Next-80B-A3B** is the first installment in the Qwen3-Next series and features the following key enchancements:\\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling for ultra-long context length.\\n- **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. \\n- **Stability Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, and other stabilizing enhancements for robust pre-training and post-training.  \\n- **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\\n\\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\\n- Qwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\\n- Qwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\\n\\n![Qwen3-Next-80B-A3B-Instruct Benchmark Comparison](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/Qwen3-Next-80B-A3B-Instruct.001.jpeg)\\n\\nFor more details, please refer to our blog post [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list).\\n\\n## Model Overview\\n\\n&gt; [!Note]\\n&gt; **Qwen3-Next-80B-A3B-Instruct** supports only instruct (non-thinking) mode and does not generate ``&lt;think&gt;&lt;/think&gt;`` blocks in its output.\\n\\n**Qwen3-Next-80B-A3B-Instruct** has the following features:\\n- Type: Causal Language Models\\n- Training Stage: Pretraining (15T tokens) &amp; Post-training\\n- Number of Parameters: 80B in total and 3B activated\\n- Number of Paramaters (Non-Embedding): 79B\\n- Hidden Dimension: 2048\\n- Number of Layers: 48\\n  - Hybrid Layout: 12 \\* (3 \\* (Gated DeltaNet -&gt; MoE) -&gt; 1 \\* (Gated Attention -&gt; MoE))\\n- Gated Attention:\\n  - Number of Attention Heads: 16 for Q and 2 for KV\\n  - Head Dimension: 256\\n  - Rotary Position Embedding Dimension: 64\\n- Gated DeltaNet:\\n  - Number of Linear Attention Heads: 32 for V and 16 for QK\\n  - Head Dimension: 128\\n- Mixture of Experts:\\n  - Number of Experts: 512\\n  - Number of Activated Experts: 10\\n  - Number of Shared Experts: 1\\n  - Expert Intermediate Dimension: 512\\n- Context Length: 262,144 natively and extensible up to 1,010,000 tokens\\n\\n&lt;img src=\"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/model_architecture.png\" height=\"384px\" title=\"Qwen3-Next Model Architecture\" /&gt;\\n\\n\\n## Performance\\n\\n|  | Qwen3-30B-A3B-Instruct-2507 | Qwen3-32B Non-Thinking | Qwen3-235B-A22B-Instruct-2507 | Qwen3-Next-80B-A3B-Instruct |\\n|--- | --- | --- | --- | --- |\\n| **Knowledge** | | | | |\\n| MMLU-Pro | 78.4 | 71.9 | **83.0** | 80.6 |\\n| MMLU-Redux | 89.3 | 85.7 | **93.1** | 90.9 |\\n| GPQA | 70.4 | 54.6 | **77.5** | 72.9 |\\n| SuperGPQA | 53.4 | 43.2 | **62.6** | 58.8 |\\n| **Reasoning** | | | | |\\n| AIME25 | 61.3 | 20.2 | **70.3** | 69.5 |\\n| HMMT25 | 43.0 | 9.8 | **55.4** | 54.1 |\\n| LiveBench 20241125 | 69.0 | 59.8 | 75.4 | **75.8** |\\n| **Coding** | | | | |\\n| LiveCodeBench v6 (25.02-25.05) | 43.2 | 29.1 | 51.8 | **56.6** |\\n| MultiPL-E | 83.8 | 76.9 | **87.9** | 87.8 |\\n| Aider-Polyglot | 35.6 | 40.0 | **57.3** | 49.8 |\\n| **Alignment** | | | | |\\n| IFEval | 84.7 | 83.2 | **88.7** | 87.6 |\\n| Arena-Hard v2* | 69.0 | 34.1 | 79.2 | **82.7** |\\n| Creative Writing v3 | 86.0 | 78.3 | **87.5** | 85.3 |\\n| WritingBench | 85.5 | 75.4 | 85.2 | **87.3** |\\n| **Agent** | | | | |\\n| BFCL-v3 | 65.1 | 63.0 | **70.9** | 70.3 |\\n| TAU1-Retail | 59.1 | 40.1 | **71.3** | 60.9 |\\n| TAU1-Airline | 40.0 | 17.0 | **44.0** | 44.0 |\\n| TAU2-Retail | 57.0 | 48.8 | **74.6** | 57.3 |\\n| TAU2-Airline | 38.0 | 24.0 | **50.0** | 45.5 |\\n| TAU2-Telecom | 12.3 | 24.6 | **32.5** | 13.2 |\\n| **Multilingualism** | | | | |\\n| MultiIF | 67.9 | 70.7 | **77.5** | 75.8 |\\n| MMLU-ProX | 72.0 | 69.3 | **79.4** | 76.7 |\\n| INCLUDE | 71.9 | 70.9 | **79.5** | 78.9 |\\n| PolyMATH | 43.1 | 22.5 | **50.2** | 45.9 |\\n\\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\\n\\n## Quickstart\\n\\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face `transformers`.\\n\\n```shell\\npip install git+https://github.com/huggingface/transformers.git@main\\n```\\n\\nWith earlier versions, you will encounter the following error:\\n```\\nKeyError: 'qwen3_next'\\n```\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    dtype=\"auto\",\\n    device_map=\"auto\",\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt},\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=16384,\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \\n\\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\\n\\nprint(\"content:\", content)\\n```\\n\\n&gt; [!Note]\\n&gt; Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\\n\\n&gt; [!Note]\\n&gt; The efficiency or throughput improvement depends highly on the implementation.\\n&gt; It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\\n\\n&gt; [!Tip]\\n&gt; Depending on the inference settings, you may observe better efficiency with [`flash-linear-attention`](https://github.com/fla-org/flash-linear-attention#installation) and [`causal-conv1d`](https://github.com/Dao-AILab/causal-conv1d).\\n&gt; See the links for detailed instructions and requirements.\\n\\n\\n## Deployment\\n\\nFor deployment, you can use the latest `sglang` or `vllm` to create an OpenAI-compatible API endpoint.\\n\\n### SGLang\\n\\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\\nSGLang could be used to launch a server with OpenAI-compatible API service. \\n\\n`sglang&gt;=0.5.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'sglang[all]&gt;=0.5.2'\\n```\\nSee [its documentation](https://docs.sglang.ai/get_started/install.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:30000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\\n```\\n\\n&gt; [!Note]\\n&gt; The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\\n\\nPlease also refer to SGLang's usage guide on [Qwen3-Next](https://docs.sglang.ai/basic_usage/qwen3.html).\\n\\n### vLLM\\n\\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\\nvLLM could be used to launch a server with OpenAI-compatible API service. \\n\\n`vllm&gt;=0.10.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'vllm&gt;=0.10.2'\\n```\\nSee [its documentation](https://docs.vllm.ai/en/stable/getting_started/installation/index.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:8000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\\n```\\n\\n&gt; [!Note]\\n&gt; The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\\n\\nPlease also refer to vLLM's usage guide on [Qwen3-Next](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html).\\n\\n## Agentic Use\\n\\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\\n\\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\\n```python\\nfrom qwen_agent.agents import Assistant\\n\\n# Define LLM\\nllm_cfg = {\\n    'model': 'Qwen3-Next-80B-A3B-Instruct',\\n\\n    # Use a custom endpoint compatible with OpenAI API:\\n    'model_server': 'http://localhost:8000/v1',  # api_base\\n    'api_key': 'EMPTY',\\n}\\n\\n# Define Tools\\ntools = [\\n    {'mcpServers': {  # You can specify the MCP configuration file\\n            'time': {\\n                'command': 'uvx',\\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\\n            },\\n            \"fetch\": {\\n                \"command\": \"uvx\",\\n                \"args\": [\"mcp-server-fetch\"]\\n            }\\n        }\\n    },\\n  'code_interpreter',  # Built-in tools\\n]\\n\\n# Define Agent\\nbot = Assistant(llm=llm_cfg, function_list=tools)\\n\\n# Streaming generation\\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\\nfor responses in bot.run(messages=messages):\\n    pass\\nprint(responses)\\n```\\n\\n\\n## Processing Ultra-Long Texts\\n\\nQwen3-Next natively supports context lengths of up to 262,144 tokens. \\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. \\nWe have validated the model's performance on context lengths of up to 1 million tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\\n\\nYaRN is currently supported by several inference frameworks, e.g., `transformers`, `vllm` and `sglang`. \\nIn general, there are two approaches to enabling YaRN for supported frameworks:\\n\\n- Modifying the model files:\\n  In the `config.json` file, add the `rope_scaling` fields:\\n    ```json\\n    {\\n        ...,\\n        \"rope_scaling\": {\\n            \"rope_type\": \"yarn\",\\n            \"factor\": 4.0,\\n            \"original_max_position_embeddings\": 262144\\n        }\\n    }\\n    ```\\n\\n- Passing command line arguments:\\n\\n  For `vllm`, you can use\\n    ```shell\\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000  \\n    ```\\n\\n  For `sglang`, you can use\\n    ```shell\\n    SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\\n    ```\\n\\n&gt; [!NOTE]\\n&gt; All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\\n&gt; We advise adding the `rope_scaling` configuration only when processing long contexts is required. \\n&gt; It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set `factor` as 2.0. \\n\\n#### Long-Context Performance\\n\\nWe test the model on an 1M version of the [RULER](https://arxiv.org/abs/2404.06654) benchmark.\\n\\n| Model Name                                  | Acc avg | 4k   | 8k   | 16k  | 32k  | 64k  | 96k  | 128k | 192k | 256k | 384k | 512k | 640k | 768k | 896k | 1000k |\\n|---------------------------------------------|---------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|-------|\\n| Qwen3-30B-A3B-Instruct-2507                 | 86.8    | 98.0 | 96.7 | 96.9 | 97.2 | 93.4 | 91.0 | 89.1 | 89.8 | 82.5 | 83.6 | 78.4 | 79.7 | 77.6 | 75.7 | 72.8  |\\n| Qwen3-235B-A22B-Instruct-2507               | 92.5    | 98.5 | 97.6 | 96.9 | 97.3 | 95.8 | 94.9 | 93.9 | 94.5 | 91.0 | 92.2 | 90.9 | 87.8 | 84.8 | 86.5 | 84.5  |\\n| Qwen3-Next-80B-A3B-Instruct                 | 91.8    | 98.5 | 99.0 | 98.0 | 98.7 | 97.6 | 95.0 | 96.0 | 94.0 | 93.5 | 91.7 | 86.9 | 85.5 | 81.7 | 80.3 | 80.3  |\\n\\n* Qwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\\n* Since the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\\n\\n## Best Practices\\n\\nTo achieve optimal performance, we recommend the following settings:\\n\\n1. **Sampling Parameters**:\\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\\n\\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\\n\\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\\n\\n### Citation\\n\\nIf you find our work helpful, feel free to give us a cite.\\n\\n```\\n@misc{qwen3technicalreport,\\n      title={Qwen3 Technical Report}, \\n      author={Qwen Team},\\n      year={2025},\\n      eprint={2505.09388},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2505.09388}, \\n}\\n\\n@article{qwen2.5-1m,\\n      title={Qwen2.5-1M Technical Report}, \\n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\\n      journal={arXiv preprint arXiv:2501.15383},\\n      year={2025}\\n}\\n```</td>\n",
       "      <td>2025-09-09T15:40:56+00:00</td>\n",
       "      <td>model</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/Qwen3-Next-80B-A3B-Instruct, tiny-random/qwen3-next-moe, kikekewl/Qwen3-Next-80B-A3B-mlx-bf16, model_quantized_model:cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit, unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit, mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit, Intel/Qwen3-Next-80B-A3B-Instruct-int4-mixed-AutoRound, nightmedia/Qwen3-Next-80B-A3B-Instruct-q2-mlx, nightmedia/Qwen3-Next-80B-A3B-Instruct-mxfp4-mlx, DevQuasar/Qwen.Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic, TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8, nightmedia/Qwen3-Next-80B-A3B-Instruct-qx86-hi-mlx, mlx-community/Qwen3-Next-80B-A3B-Instruct-5bit, mlx-community/Qwen3-Next-80B-A3B-Instruct-6bit, mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit, TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen/Qwen3-Next-80B-A3B-Thinking</td>\n",
       "      <td>---\\nlibrary_name: transformers\\nlicense: apache-2.0\\nlicense_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking/blob/main/LICENSE\\npipeline_tag: text-generation\\n---\\n\\n# Qwen3-Next-80B-A3B-Thinking\\n&lt;a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n&lt;/a&gt;\\n\\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). \\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. \\nWe call this next-generation foundation models **Qwen3-Next**.\\n\\n## Highlights\\n\\n**Qwen3-Next-80B-A3B** is the first installment in the Qwen3-Next series and features the following key enchancements:\\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling for ultra-long context length.\\n- **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. \\n- **Stability Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, and other stabilizing enhancements for robust pre-training and post-training.  \\n- **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\\n\\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\\n- Qwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\\n- Leveraging [GSPO](https://qwenlm.github.io/blog/gspo/), we have addressed the stability and efficiency challenges posed by the hybrid attention mechanism combined with a high-sparsity MoE architecture in RL training. \\n  Qwen3-Next-80B-A3B-Thinking demonstrates outstanding performance on complex reasoning tasks, not only **surpassing Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B-Thinking**, but also **outperforming the proprietary model Gemini-2.5-Flash-Thinking** across multiple benchmarks.\\n\\n![Qwen3-Next-80B-A3B-Thinking Benchmark Comparison](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/Qwen3-Next-80B-A3B-Thinking.001.jpeg)\\n\\nFor more details, please refer to our blog post [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list).\\n\\n## Model Overview\\n\\n&gt; [!Note]\\n&gt; **Qwen3-Next-80B-A3B-Thinking** supports only thinking mode. \\n&gt; To enforce model thinking, the default chat template automatically includes `&lt;think&gt;`. \\n&gt; Therefore, it is normal for the model's output to contain only `&lt;/think&gt;` without an explicit opening `&lt;think&gt;` tag.\\n\\n&gt; [!Note]\\n&gt; **Qwen3-Next-80B-A3B-Thinking** may generate thinking content longer than its predecessor.\\n&gt; We strongly recommend its use in highly complex reasoning tasks.\\n\\n\\n**Qwen3-Next-80B-A3B-Thinking** has the following features:\\n- Type: Causal Language Models\\n- Training Stage: Pretraining (15T tokens) &amp; Post-training\\n- Number of Parameters: 80B in total and 3B activated\\n- Number of Paramaters (Non-Embedding): 79B\\n- Hidden Dimension: 2048\\n- Number of Layers: 48\\n  - Hybrid Layout: 12 \\* (3 \\* (Gated DeltaNet -&gt; MoE) -&gt; 1 \\* (Gated Attention -&gt; MoE))\\n- Gated Attention:\\n  - Number of Attention Heads: 16 for Q and 2 for KV\\n  - Head Dimension: 256\\n  - Rotary Position Embedding Dimension: 64\\n- Gated DeltaNet:\\n  - Number of Linear Attention Heads: 32 for V and 16 for QK\\n  - Head Dimension: 128\\n- Mixture of Experts:\\n  - Number of Experts: 512\\n  - Number of Activated Experts: 10\\n  - Number of Shared Experts: 1\\n  - Expert Intermediate Dimension: 512\\n- Context Length: 262,144 natively and extensible up to 1,010,000 tokens\\n\\n&lt;img src=\"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/model_architecture.png\" height=\"384px\" title=\"Qwen3-Next Model Architecture\" /&gt;\\n\\n\\n## Performance\\n\\n|  | Qwen3-30B-A3B-Thinking-2507 | Qwen3-32B Thinking | Qwen3-235B-A22B-Thinking-2507 | Gemini-2.5-Flash Thinking | Qwen3-Next-80B-A3B-Thinking |\\n|--- | --- | --- | --- | --- | --- |\\n| **Knowledge** | | | | |\\n| MMLU-Pro | 80.9 | 79.1 | **84.4** | 81.9 | 82.7 |\\n| MMLU-Redux | 91.4 | 90.9 | **93.8** | 92.1 | 92.5 |\\n| GPQA | 73.4 | 68.4 | 81.1 | **82.8** | 77.2 |\\n| SuperGPQA | 56.8 | 54.1 | **64.9** | 57.8 | 60.8 |\\n| **Reasoning** | | | | |\\n| AIME25 | 85.0 | 72.9 | **92.3** | 72.0 | 87.8 |\\n| HMMT25 | 71.4 | 51.5 | **83.9** | 64.2 | 73.9 |\\n| LiveBench 241125 | 76.8 | 74.9 | **78.4** | 74.3 | 76.6 |\\n| **Coding** | | | | |\\n| LiveCodeBench v6 (25.02-25.05) | 66.0 | 60.6 | **74.1** | 61.2 | 68.7 |\\n| CFEval | 2044 | 1986 | **2134** | 1995 | 2071 |\\n| OJBench | 25.1 | 24.1 | **32.5** | 23.5 | 29.7 |\\n| **Alignment** | | | | |\\n| IFEval | 88.9 | 85.0 | 87.8 | **89.8** | 88.9 |\\n| Arena-Hard v2* | 56.0 | 48.4 | **79.7** | 56.7 | 62.3 |\\n| WritingBench | 85.0 | 79.0 | **88.3** | 83.9 | 84.6 |\\n| **Agent** | | | | |\\n| BFCL-v3 |  **72.4** | 70.3 | 71.9 | 68.6 | 72.0 |\\n| TAU1-Retail | 67.8 | 52.8 | 67.8 | 65.2 | **69.6** |\\n| TAU1-Airline | 48.0 | 29.0 | 46.0 | **54.0** | 49.0 |\\n| TAU2-Retail | 58.8 | 49.7 | **71.9** | 66.7 | 67.8 |\\n| TAU2-Airline | 58.0 | 45.5 | 58.0 | 52.0 | **60.5** |\\n| TAU2-Telecom | 26.3 | 27.2 | **45.6** | 31.6 | 43.9 |\\n| **Multilingualism** | | | | |\\n| MultiIF | 76.4 | 73.0 | **80.6** | 74.4 | 77.8 |\\n| MMLU-ProX | 76.4 | 74.6 | **81.0** | 80.2 | 78.7 |\\n| INCLUDE | 74.4 | 73.7 | 81.0 | **83.9** | 78.9 |\\n| PolyMATH | 52.6 | 47.4 | **60.1** | 49.8 | 56.3 |\\n\\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\\n\\n## Quickstart\\n\\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face `transformers`. \\n\\n```shell\\npip install git+https://github.com/huggingface/transformers.git@main\\n```\\n\\nWith earlier versions, you will encounter the following error:\\n```\\nKeyError: 'qwen3_next'\\n```\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt},\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=32768,\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \\n\\n# parsing thinking content\\ntry:\\n    # rindex finding 151668 (&lt;/think&gt;)\\n    index = len(output_ids) - output_ids[::-1].index(151668)\\nexcept ValueError:\\n    index = 0\\n\\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\\n\\nprint(\"thinking content:\", thinking_content) # no opening &lt;think&gt; tag\\nprint(\"content:\", content)\\n```\\n\\n&gt; [!Note]\\n&gt; Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\\n\\n&gt; [!Note]\\n&gt; The efficiency or throughput improvement depends highly on the implementation.\\n&gt; It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\\n\\n&gt; [!Tip]\\n&gt; Depending on the inference settings, you may observe better efficiency with [`flash-linear-attention`](https://github.com/fla-org/flash-linear-attention#installation) and [`causal-conv1d`](https://github.com/Dao-AILab/causal-conv1d).\\n&gt; See the links for detailed instructions and requirements.\\n\\n## Deployment\\n\\nFor deployment, you can use the latest `sglang` or `vllm` to create an OpenAI-compatible API endpoint.\\n\\n### SGLang\\n\\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\\nSGLang could be used to launch a server with OpenAI-compatible API service. \\n\\n`sglang&gt;=0.5.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'sglang[all]&gt;=0.5.2'\\n```\\nSee [its documentation](https://docs.sglang.ai/get_started/install.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:30000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Thinking --port 30000 --tp-size 4 --context-length 262144 --reasoning-parser deepseek-r1 --mem-fraction-static 0.8\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Thinking --port 30000 --tp-size 4 --context-length 262144 --reasoning-parser deepseek-r1 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\\n```\\n\\n&gt; [!Note]\\n&gt; The default context length is 256K. \\n&gt; If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. \\n&gt; However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072.\\n\\nPlease also refer to SGLang's usage guide on [Qwen3-Next](https://docs.sglang.ai/basic_usage/qwen3.html).\\n\\n### vLLM\\n\\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\\nvLLM could be used to launch a server with OpenAI-compatible API service. \\n\\n`vllm&gt;=0.10.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'vllm&gt;=0.10.2'\\n```\\nSee [its documentation](https://docs.vllm.ai/en/stable/getting_started/installation/index.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:8000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --reasoning-parser deepseek_r1\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --reasoning-parser deepseek_r1 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\\n```\\n\\n&gt; [!Note]\\n&gt; The default context length is 256K. \\n&gt; If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. \\n&gt; However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\\n\\nPlease also refer to vLLM's usage guide on [Qwen3-Next](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html).\\n\\n\\n## Agentic Use\\n\\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\\n\\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\\n```python\\nfrom qwen_agent.agents import Assistant\\n\\n# Define LLM\\n# Using Alibaba Cloud Model Studio\\nllm_cfg = {\\n    'model': 'Qwen3-Next-80B-A3B-Thinking',\\n    'model_type': 'qwen_dashscope',\\n}\\n\\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example, \\n# `vllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --served-model-name Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144`.\\n#\\n# llm_cfg = {\\n#     'model': 'Qwen3-Next-80B-A3B-Thinking',\\n# \\n#     # Use a custom endpoint compatible with OpenAI API:\\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\\n#     'api_key': 'EMPTY',\\n#     'generate_cfg': {\\n#         'thought_in_content': True,\\n#     },\\n# }\\n\\n# Define Tools\\ntools = [\\n    {'mcpServers': {  # You can specify the MCP configuration file\\n            'time': {\\n                'command': 'uvx',\\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\\n            },\\n            \"fetch\": {\\n                \"command\": \"uvx\",\\n                \"args\": [\"mcp-server-fetch\"]\\n            }\\n        }\\n    },\\n  'code_interpreter',  # Built-in tools\\n]\\n\\n# Define Agent\\nbot = Assistant(llm=llm_cfg, function_list=tools)\\n\\n# Streaming generation\\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\\nfor responses in bot.run(messages=messages):\\n    pass\\nprint(responses)\\n```\\n\\n\\n## Processing Ultra-Long Texts\\n\\nQwen3-Next natively supports context lengths of up to 262,144 tokens. \\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. \\nWe have validated the model's performance on context lengths of up to 1 million tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\\n\\nYaRN is currently supported by several inference frameworks, e.g., `transformers`, `vllm` and `sglang`. \\nIn general, there are two approaches to enabling YaRN for supported frameworks:\\n\\n- Modifying the model files:\\n  In the `config.json` file, add the `rope_scaling` fields:\\n    ```json\\n    {\\n        ...,\\n        \"rope_scaling\": {\\n            \"rope_type\": \"yarn\",\\n            \"factor\": 4.0,\\n            \"original_max_position_embeddings\": 262144\\n        }\\n    }\\n    ```\\n\\n- Passing command line arguments:\\n\\n  For `vllm`, you can use\\n    ```shell\\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000  \\n    ```\\n\\n  For `sglang`, you can use\\n    ```shell\\n    SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\\n    ```\\n\\n&gt; [!NOTE]\\n&gt; All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\\n&gt; We advise adding the `rope_scaling` configuration only when processing long contexts is required. \\n&gt; It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set `factor` as 2.0. \\n\\n## Best Practices\\n\\nTo achieve optimal performance, we recommend the following settings:\\n\\n1. **Sampling Parameters**:\\n   - We suggest using `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`.\\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\\n\\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\\n\\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\\n\\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\\n\\n### Citation\\n\\nIf you find our work helpful, feel free to give us a cite.\\n\\n```\\n@misc{qwen3technicalreport,\\n      title={Qwen3 Technical Report}, \\n      author={Qwen Team},\\n      year={2025},\\n      eprint={2505.09388},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2505.09388}, \\n}\\n\\n@article{qwen2.5-1m,\\n      title={Qwen2.5-1M Technical Report}, \\n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\\n      journal={arXiv preprint arXiv:2501.15383},\\n      year={2025}\\n}\\n```</td>\n",
       "      <td>2025-09-09T15:45:31+00:00</td>\n",
       "      <td>model</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:kikekewl/Qwen3-Next-80B-A3B-Thinking-mlx-bf16, model_quantized_model:Intel/Qwen3-Next-80B-A3B-Thinking-int4-mixed-AutoRound, cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit, llllwxxx/Qwen3-Next-80B-A3B-Thinking-FP8-Dynamic, mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google/embeddinggemma-300m</td>\n",
       "      <td>---\\nlicense: gemma\\npipeline_tag: sentence-similarity\\nlibrary_name: sentence-transformers\\ntags:\\n- sentence-transformers\\n- sentence-similarity\\n- feature-extraction\\n- text-embeddings-inference\\nextra_gated_heading: Access EmbeddingGemma on Hugging Face\\nextra_gated_prompt: To access EmbeddingGemma on Hugging Face, you‚Äôre required to review and\\n  agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging\\n  Face and click below. Requests are processed immediately.\\nextra_gated_button_content: Acknowledge license\\n---\\n\\n# EmbeddingGemma model card\\n\\n**Model Page**: [EmbeddingGemma](https://ai.google.dev/gemma/docs/embeddinggemma)\\n\\n**Resources and Technical Documentation**:\\n\\n*   [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\\n*   [EmbeddingGemma on Kaggle](https://www.kaggle.com/models/google/embeddinggemma/)\\n*   [EmbeddingGemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/embeddinggemma)\\n\\n**Terms of Use**: [Terms](https://ai.google.dev/gemma/terms)\\n\\n**Authors**: Google DeepMind\\n\\n## Model Information\\n\\n### Description\\n\\nEmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.\\n\\nThe small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone.\\n\\n### Inputs and outputs\\n\\n-   **Input:**\\n    -   Text string, such as a question, a prompt, or a document to be embedded\\n    -   Maximum input context length of 2048 tokens\\n\\n-   **Output:**\\n    -   Numerical vector representations of input text data\\n    -   Output embedding dimension size of 768, with smaller options available (512, 256, or 128) via Matryoshka Representation Learning (MRL). MRL allows users to truncate the output embedding of size 768 to their desired size and then re-normalize for efficient and accurate representation.\\n\\n### Usage\\n\\nThese model weights are designed to be used with [Sentence Transformers](https://www.SBERT.net), using the [Gemma 3](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3) implementation from [Hugging Face Transformers](https://huggingface.co/docs/transformers/en/index) as the backbone.\\n\\nFirst install the Sentence Transformers library:\\n\\n```bash\\npip install -U sentence-transformers\\n```\\n\\nThen you can load this model and run inference.\\n\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\n# Download from the ü§ó Hub\\nmodel = SentenceTransformer(\"google/embeddinggemma-300m\")\\n\\n# Run inference with queries and documents\\nquery = \"Which planet is known as the Red Planet?\"\\ndocuments = [\\n    \"Venus is often called Earth's twin because of its similar size and proximity.\",\\n    \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\\n    \"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\\n    \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\\n]\\nquery_embeddings = model.encode_query(query)\\ndocument_embeddings = model.encode_document(documents)\\nprint(query_embeddings.shape, document_embeddings.shape)\\n# (768,) (4, 768)\\n\\n# Compute similarities to determine a ranking\\nsimilarities = model.similarity(query_embeddings, document_embeddings)\\nprint(similarities)\\n# tensor([[0.3011, 0.6359, 0.4930, 0.4889]])\\n```\\n\\n**NOTE**: EmbeddingGemma activations do not support `float16`. Please use `float32` or `bfloat16` as appropriate for your hardware.\\n\\n## Model Data\\n\\n### Training Dataset\\n\\nThis model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components:\\n\\n-   **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages.\\n-   **Code and Technical Documents**: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.\\n-   **Synthetic and Task-Specific Data**: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications.\\n\\nThe combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats.\\n\\n### Data Preprocessing\\n\\nHere are the key data cleaning and filtering methods applied to the training data:\\n\\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content.\\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.\\n-   Additional methods: Filtering based on content quality and safety in line with [our policies](https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf).\\n\\n## Model Development\\n\\n### Hardware\\n\\nEmbeddingGemma was trained using the latest generation of [Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv5e), for more details refer to the [Gemma 3 model card](https://ai.google.dev/gemma/docs/core/model_card_3).\\n\\n### Software\\n\\nTraining was done using [JAX](https://github.com/jax-ml/jax) and [ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/). For more details refer to the [Gemma 3 model card](https://ai.google.dev/gemma/docs/core/model_card_3).\\n\\n## Evaluation\\n\\n### Benchmark Results\\n\\nThe model was evaluated against a large collection of different datasets and metrics to cover different aspects of text understanding.\\n\\n#### Full Precision Checkpoint\\n\\n&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr&gt;\\n      &lt;th colspan=\"3\"&gt;&lt;strong&gt;MTEB (Multilingual, v2)&lt;/strong&gt;&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;strong&gt;Dimensionality&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (Task)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (TaskType)&lt;/strong&gt;&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;768d&lt;/td&gt;\\n      &lt;td&gt;61.15&lt;/td&gt;\\n      &lt;td&gt;54.31&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;512d&lt;/td&gt;\\n      &lt;td&gt;60.71&lt;/td&gt;\\n      &lt;td&gt;53.89&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;256d&lt;/td&gt;\\n      &lt;td&gt;59.68&lt;/td&gt;\\n      &lt;td&gt;53.01&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;128d&lt;/td&gt;\\n      &lt;td&gt;58.23&lt;/td&gt;\\n      &lt;td&gt;51.77&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\\n&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr&gt;\\n      &lt;th colspan=\"3\"&gt;&lt;strong&gt;MTEB (English, v2)&lt;/strong&gt;&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;strong&gt;Dimensionality&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (Task)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (TaskType)&lt;/strong&gt;&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;768d&lt;/td&gt;\\n      &lt;td&gt;68.36&lt;/td&gt;\\n      &lt;td&gt;64.15&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;512d&lt;/td&gt;\\n      &lt;td&gt;67.80&lt;/td&gt;\\n      &lt;td&gt;63.59&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;256d&lt;/td&gt;\\n      &lt;td&gt;66.89&lt;/td&gt;\\n      &lt;td&gt;62.94&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;128d&lt;/td&gt;\\n      &lt;td&gt;65.09&lt;/td&gt;\\n      &lt;td&gt;61.56&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\\n&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr&gt;\\n      &lt;th colspan=\"3\"&gt;&lt;strong&gt;MTEB (Code, v1)&lt;/strong&gt;&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;strong&gt;Dimensionality&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (Task)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (TaskType)&lt;/strong&gt;&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;768d&lt;/td&gt;\\n      &lt;td&gt;68.76&lt;/td&gt;\\n      &lt;td&gt;68.76&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;512d&lt;/td&gt;\\n      &lt;td&gt;68.48&lt;/td&gt;\\n      &lt;td&gt;68.48&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;256d&lt;/td&gt;\\n      &lt;td&gt;66.74&lt;/td&gt;\\n      &lt;td&gt;66.74&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;128d&lt;/td&gt;\\n      &lt;td&gt;62.96&lt;/td&gt;\\n      &lt;td&gt;62.96&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\\n#### QAT Checkpoints\\n\\n&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr&gt;\\n      &lt;th colspan=\"3\"&gt;&lt;strong&gt;MTEB (Multilingual, v2)&lt;/strong&gt;&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;strong&gt;Quant config (dimensionality)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (Task)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (TaskType)&lt;/strong&gt;&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Q4_0 (768d)&lt;/td&gt;\\n      &lt;td&gt;60.62&lt;/td&gt;\\n      &lt;td&gt;53.61&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Q8_0 (768d)&lt;/td&gt;\\n      &lt;td&gt;60.93&lt;/td&gt;\\n      &lt;td&gt;53.95&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Mixed Precision* (768d)&lt;/td&gt;\\n      &lt;td&gt;60.69&lt;/td&gt;\\n      &lt;td&gt;53.82&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\\n&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr&gt;\\n      &lt;th colspan=\"3\"&gt;&lt;strong&gt;MTEB (English, v2)&lt;/strong&gt;&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;strong&gt;Quant config (dimensionality)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (Task)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (TaskType)&lt;/strong&gt;&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Q4_0 (768d)&lt;/td&gt;\\n      &lt;td&gt;67.91&lt;/td&gt;\\n      &lt;td&gt;63.64&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Q8_0 (768d)&lt;/td&gt;\\n      &lt;td&gt;68.13&lt;/td&gt;\\n      &lt;td&gt;63.85&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Mixed Precision* (768d)&lt;/td&gt;\\n      &lt;td&gt;67.95&lt;/td&gt;\\n      &lt;td&gt;63.83&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\\n&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr&gt;\\n      &lt;th colspan=\"3\"&gt;&lt;strong&gt;MTEB (Code, v1)&lt;/strong&gt;&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;strong&gt;Quant config (dimensionality)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (Task)&lt;/strong&gt;&lt;/td&gt;\\n      &lt;td&gt;&lt;strong&gt;Mean (TaskType)&lt;/strong&gt;&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Q4_0 (768d)&lt;/td&gt;\\n      &lt;td&gt;67.99&lt;/td&gt;\\n      &lt;td&gt;67.99&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Q8_0 (768d)&lt;/td&gt;\\n      &lt;td&gt;68.70&lt;/td&gt;\\n      &lt;td&gt;68.70&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Mixed Precision* (768d)&lt;/td&gt;\\n      &lt;td&gt;68.03&lt;/td&gt;\\n      &lt;td&gt;68.03&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\\nNote: QAT models are evaluated after quantization\\n\\n\\* Mixed Precision refers to per-channel quantization with int4 for embeddings, feedforward, and projection layers, and int8 for attention (e4_a8_f4_p4).\\n\\n### Prompt Instructions\\n\\nEmbeddingGemma can generate optimized embeddings for various use cases‚Äîsuch as document retrieval, question answering, and fact verification‚Äîor for specific input types‚Äîeither a query or a document‚Äîusing prompts that are prepended to the input strings.\\nQuery prompts follow the form `task: {task description} | query: ` where the task description varies by the use case, with the default task description being `search result`. Document-style prompts follow the form `title: {title | \"none\"} | text: ` where the title is either `none` (the default) or the actual title of the document. Note that providing a title, if available, will improve model performance for document prompts but may require manual formatting.\\n\\nUse the following prompts based on your use case and input data type. These may already be available in the EmbeddingGemma configuration in your modeling framework of choice.\\n\\n&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;&lt;br&gt;\\n&lt;strong&gt;Use Case (task type enum)&lt;/strong&gt;&lt;/th&gt;\\n      &lt;th&gt;&lt;br&gt;\\n&lt;strong&gt;Descriptions&lt;/strong&gt;&lt;/th&gt;\\n      &lt;th&gt;&lt;br&gt;\\n&lt;strong&gt;Recommended Prompt&lt;/strong&gt;&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nRetrieval (Query)&lt;/td&gt;\\n      &lt;td rowspan=\"4\"&gt;&lt;br&gt;\\nUsed to generate embeddings that are optimized for document search or information retrieval&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntask: search result | query: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nRetrieval (Document)&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntitle: {title | \"none\"} | text: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nQuestion Answering&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntask: question answering | query: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nFact Verification&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntask: fact checking | query: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nClassification&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\nUsed to generate embeddings that are optimized to classify texts according to preset labels&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntask: classification | query: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nClustering&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\nUsed to generate embeddings that are optimized to cluster texts based on their similarities&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntask: clustering | query: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nSemantic Similarity&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\nUsed to generate embeddings that are optimized to assess text similarity. This is not intended for retrieval use cases.&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntask: sentence similarity | query: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;&lt;br&gt;\\nCode Retrieval&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\nUsed to retrieve a code block based on a natural language query, such as &lt;em&gt;sort an array&lt;/em&gt; or &lt;em&gt;reverse a linked list&lt;/em&gt;. Embeddings of the code blocks are computed using retrieval_document.&lt;/td&gt;\\n      &lt;td&gt;&lt;br&gt;\\ntask: code retrieval | query: {content}&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\\n## Usage and Limitations\\n\\nThese models have certain limitations that users should be aware of.\\n\\n### Intended Usage\\n\\nOpen embedding models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.\\n\\n-   **Semantic Similarity**: Embeddings optimized to assess text similarity, such as recommendation systems and duplicate detection\\n-   **Classification**: Embeddings optimized to classify texts according to preset labels, such as sentiment analysis and spam detection\\n-  **Clustering**: Embeddings optimized to cluster texts based on their similarities, such as document organization, market research, and anomaly detection\\n-   **Retrieval**\\n    -   **Document**: Embeddings optimized for document search, such as indexing articles, books, or web pages for search\\n    -   **Query**: Embeddings optimized for general search queries, such as custom search\\n    -   **Code Query**: Embeddings optimized for retrieval of code blocks based on natural language queries, such as code suggestions and search\\n\\n-   **Question Answering**: Embeddings for questions in a question-answering system, optimized for finding documents that answer the question, such as chatbox.\\n-   **Fact Verification**: Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement, such as automated fact-checking systems.\\n\\n### Limitations\\n\\n-   Training Data\\n    -   The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.\\n    -   The scope of the training dataset determines the subject areas the model can handle effectively.\\n\\n-   Language Ambiguity and Nuance\\n    -   Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language.\\n\\n### Ethical Considerations and Risks\\n\\nRisks identified and mitigations:\\n\\n-   **Perpetuation of biases**: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases.\\n-   **Misuse for malicious purposes**: Technical limitations and developer and end-user education can help mitigate against malicious applications of embeddings. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in the [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\\n-   **Privacy violations**: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\\n\\n### Benefits\\n\\nAt the time of release, this family of models provides high-performance open embedding model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown superior performance to other, comparably-sized open model alternatives.</td>\n",
       "      <td>2025-07-17T19:53:55+00:00</td>\n",
       "      <td>model</td>\n",
       "      <td>[24, 7]</td>\n",
       "      <td>model_finetune_model:sigridjineth/colbert-ko-embeddinggemma-300m, sentence-transformers/embeddinggemma-300m-medical, yasserrmd/geo-gemma-300m-emb, unsloth/embeddinggemma-300m, Omartificial-Intelligence-Space/AraGemma-Embedding-300m, Saidakmal/uz_embeddinggemma-300m, yasserrmd/pharma-gemma-300m-emb, model_quantized_model:onnx-community/embeddinggemma-300m-ONNX, unsloth/embeddinggemma-300m-GGUF, ggml-org/embeddinggemma-300M-GGUF, SandLogicTechnologies/EmbeddingGemma-300m-GGUF</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0                      tencent/SRPO   \n",
       "1  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "2  Qwen/Qwen3-Next-80B-A3B-Instruct   \n",
       "3  Qwen/Qwen3-Next-80B-A3B-Thinking   \n",
       "4        google/embeddinggemma-300m   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ---\\nlibrary_name: transformers\\nlicense: apache-2.0\\nlicense_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE\\npipeline_tag: text-generation\\n---\\n\\n# Qwen3-Next-80B-A3B-Instruct\\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\\n</a>\\n\\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). \\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. \\nWe call this next-generation foundation models **Qwen3-Next**.\\n\\n## Highlights\\n\\n**Qwen3-Next-80B-A3B** is the first installment in the Qwen3-Next series and features the following key enchancements:\\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling for ultra-long context length.\\n- **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. \\n- **Stability Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, and other stabilizing enhancements for robust pre-training and post-training.  \\n- **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\\n\\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\\n- Qwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\\n- Qwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\\n\\n![Qwen3-Next-80B-A3B-Instruct Benchmark Comparison](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/Qwen3-Next-80B-A3B-Instruct.001.jpeg)\\n\\nFor more details, please refer to our blog post [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list).\\n\\n## Model Overview\\n\\n> [!Note]\\n> **Qwen3-Next-80B-A3B-Instruct** supports only instruct (non-thinking) mode and does not generate ``<think></think>`` blocks in its output.\\n\\n**Qwen3-Next-80B-A3B-Instruct** has the following features:\\n- Type: Causal Language Models\\n- Training Stage: Pretraining (15T tokens) & Post-training\\n- Number of Parameters: 80B in total and 3B activated\\n- Number of Paramaters (Non-Embedding): 79B\\n- Hidden Dimension: 2048\\n- Number of Layers: 48\\n  - Hybrid Layout: 12 \\* (3 \\* (Gated DeltaNet -> MoE) -> 1 \\* (Gated Attention -> MoE))\\n- Gated Attention:\\n  - Number of Attention Heads: 16 for Q and 2 for KV\\n  - Head Dimension: 256\\n  - Rotary Position Embedding Dimension: 64\\n- Gated DeltaNet:\\n  - Number of Linear Attention Heads: 32 for V and 16 for QK\\n  - Head Dimension: 128\\n- Mixture of Experts:\\n  - Number of Experts: 512\\n  - Number of Activated Experts: 10\\n  - Number of Shared Experts: 1\\n  - Expert Intermediate Dimension: 512\\n- Context Length: 262,144 natively and extensible up to 1,010,000 tokens\\n\\n<img src=\"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/model_architecture.png\" height=\"384px\" title=\"Qwen3-Next Model Architecture\" />\\n\\n\\n## Performance\\n\\n|  | Qwen3-30B-A3B-Instruct-2507 | Qwen3-32B Non-Thinking | Qwen3-235B-A22B-Instruct-2507 | Qwen3-Next-80B-A3B-Instruct |\\n|--- | --- | --- | --- | --- |\\n| **Knowledge** | | | | |\\n| MMLU-Pro | 78.4 | 71.9 | **83.0** | 80.6 |\\n| MMLU-Redux | 89.3 | 85.7 | **93.1** | 90.9 |\\n| GPQA | 70.4 | 54.6 | **77.5** | 72.9 |\\n| SuperGPQA | 53.4 | 43.2 | **62.6** | 58.8 |\\n| **Reasoning** | | | | |\\n| AIME25 | 61.3 | 20.2 | **70.3** | 69.5 |\\n| HMMT25 | 43.0 | 9.8 | **55.4** | 54.1 |\\n| LiveBench 20241125 | 69.0 | 59.8 | 75.4 | **75.8** |\\n| **Coding** | | | | |\\n| LiveCodeBench v6 (25.02-25.05) | 43.2 | 29.1 | 51.8 | **56.6** |\\n| MultiPL-E | 83.8 | 76.9 | **87.9** | 87.8 |\\n| Aider-Polyglot | 35.6 | 40.0 | **57.3** | 49.8 |\\n| **Alignment** | | | | |\\n| IFEval | 84.7 | 83.2 | **88.7** | 87.6 |\\n| Arena-Hard v2* | 69.0 | 34.1 | 79.2 | **82.7** |\\n| Creative Writing v3 | 86.0 | 78.3 | **87.5** | 85.3 |\\n| WritingBench | 85.5 | 75.4 | 85.2 | **87.3** |\\n| **Agent** | | | | |\\n| BFCL-v3 | 65.1 | 63.0 | **70.9** | 70.3 |\\n| TAU1-Retail | 59.1 | 40.1 | **71.3** | 60.9 |\\n| TAU1-Airline | 40.0 | 17.0 | **44.0** | 44.0 |\\n| TAU2-Retail | 57.0 | 48.8 | **74.6** | 57.3 |\\n| TAU2-Airline | 38.0 | 24.0 | **50.0** | 45.5 |\\n| TAU2-Telecom | 12.3 | 24.6 | **32.5** | 13.2 |\\n| **Multilingualism** | | | | |\\n| MultiIF | 67.9 | 70.7 | **77.5** | 75.8 |\\n| MMLU-ProX | 72.0 | 69.3 | **79.4** | 76.7 |\\n| INCLUDE | 71.9 | 70.9 | **79.5** | 78.9 |\\n| PolyMATH | 43.1 | 22.5 | **50.2** | 45.9 |\\n\\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\\n\\n## Quickstart\\n\\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face `transformers`.\\n\\n```shell\\npip install git+https://github.com/huggingface/transformers.git@main\\n```\\n\\nWith earlier versions, you will encounter the following error:\\n```\\nKeyError: 'qwen3_next'\\n```\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    dtype=\"auto\",\\n    device_map=\"auto\",\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt},\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=16384,\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \\n\\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\\n\\nprint(\"content:\", content)\\n```\\n\\n> [!Note]\\n> Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\\n\\n> [!Note]\\n> The efficiency or throughput improvement depends highly on the implementation.\\n> It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\\n\\n> [!Tip]\\n> Depending on the inference settings, you may observe better efficiency with [`flash-linear-attention`](https://github.com/fla-org/flash-linear-attention#installation) and [`causal-conv1d`](https://github.com/Dao-AILab/causal-conv1d).\\n> See the links for detailed instructions and requirements.\\n\\n\\n## Deployment\\n\\nFor deployment, you can use the latest `sglang` or `vllm` to create an OpenAI-compatible API endpoint.\\n\\n### SGLang\\n\\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\\nSGLang could be used to launch a server with OpenAI-compatible API service. \\n\\n`sglang>=0.5.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'sglang[all]>=0.5.2'\\n```\\nSee [its documentation](https://docs.sglang.ai/get_started/install.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:30000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\\n```\\n\\n> [!Note]\\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\\n\\nPlease also refer to SGLang's usage guide on [Qwen3-Next](https://docs.sglang.ai/basic_usage/qwen3.html).\\n\\n### vLLM\\n\\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\\nvLLM could be used to launch a server with OpenAI-compatible API service. \\n\\n`vllm>=0.10.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'vllm>=0.10.2'\\n```\\nSee [its documentation](https://docs.vllm.ai/en/stable/getting_started/installation/index.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:8000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\\n```\\n\\n> [!Note]\\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\\n\\nPlease also refer to vLLM's usage guide on [Qwen3-Next](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html).\\n\\n## Agentic Use\\n\\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\\n\\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\\n```python\\nfrom qwen_agent.agents import Assistant\\n\\n# Define LLM\\nllm_cfg = {\\n    'model': 'Qwen3-Next-80B-A3B-Instruct',\\n\\n    # Use a custom endpoint compatible with OpenAI API:\\n    'model_server': 'http://localhost:8000/v1',  # api_base\\n    'api_key': 'EMPTY',\\n}\\n\\n# Define Tools\\ntools = [\\n    {'mcpServers': {  # You can specify the MCP configuration file\\n            'time': {\\n                'command': 'uvx',\\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\\n            },\\n            \"fetch\": {\\n                \"command\": \"uvx\",\\n                \"args\": [\"mcp-server-fetch\"]\\n            }\\n        }\\n    },\\n  'code_interpreter',  # Built-in tools\\n]\\n\\n# Define Agent\\nbot = Assistant(llm=llm_cfg, function_list=tools)\\n\\n# Streaming generation\\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\\nfor responses in bot.run(messages=messages):\\n    pass\\nprint(responses)\\n```\\n\\n\\n## Processing Ultra-Long Texts\\n\\nQwen3-Next natively supports context lengths of up to 262,144 tokens. \\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. \\nWe have validated the model's performance on context lengths of up to 1 million tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\\n\\nYaRN is currently supported by several inference frameworks, e.g., `transformers`, `vllm` and `sglang`. \\nIn general, there are two approaches to enabling YaRN for supported frameworks:\\n\\n- Modifying the model files:\\n  In the `config.json` file, add the `rope_scaling` fields:\\n    ```json\\n    {\\n        ...,\\n        \"rope_scaling\": {\\n            \"rope_type\": \"yarn\",\\n            \"factor\": 4.0,\\n            \"original_max_position_embeddings\": 262144\\n        }\\n    }\\n    ```\\n\\n- Passing command line arguments:\\n\\n  For `vllm`, you can use\\n    ```shell\\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000  \\n    ```\\n\\n  For `sglang`, you can use\\n    ```shell\\n    SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\\n    ```\\n\\n> [!NOTE]\\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \\n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set `factor` as 2.0. \\n\\n#### Long-Context Performance\\n\\nWe test the model on an 1M version of the [RULER](https://arxiv.org/abs/2404.06654) benchmark.\\n\\n| Model Name                                  | Acc avg | 4k   | 8k   | 16k  | 32k  | 64k  | 96k  | 128k | 192k | 256k | 384k | 512k | 640k | 768k | 896k | 1000k |\\n|---------------------------------------------|---------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|-------|\\n| Qwen3-30B-A3B-Instruct-2507                 | 86.8    | 98.0 | 96.7 | 96.9 | 97.2 | 93.4 | 91.0 | 89.1 | 89.8 | 82.5 | 83.6 | 78.4 | 79.7 | 77.6 | 75.7 | 72.8  |\\n| Qwen3-235B-A22B-Instruct-2507               | 92.5    | 98.5 | 97.6 | 96.9 | 97.3 | 95.8 | 94.9 | 93.9 | 94.5 | 91.0 | 92.2 | 90.9 | 87.8 | 84.8 | 86.5 | 84.5  |\\n| Qwen3-Next-80B-A3B-Instruct                 | 91.8    | 98.5 | 99.0 | 98.0 | 98.7 | 97.6 | 95.0 | 96.0 | 94.0 | 93.5 | 91.7 | 86.9 | 85.5 | 81.7 | 80.3 | 80.3  |\\n\\n* Qwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\\n* Since the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\\n\\n## Best Practices\\n\\nTo achieve optimal performance, we recommend the following settings:\\n\\n1. **Sampling Parameters**:\\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\\n\\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\\n\\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\\n\\n### Citation\\n\\nIf you find our work helpful, feel free to give us a cite.\\n\\n```\\n@misc{qwen3technicalreport,\\n      title={Qwen3 Technical Report}, \\n      author={Qwen Team},\\n      year={2025},\\n      eprint={2505.09388},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2505.09388}, \\n}\\n\\n@article{qwen2.5-1m,\\n      title={Qwen2.5-1M Technical Report}, \\n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\\n      journal={arXiv preprint arXiv:2501.15383},\\n      year={2025}\\n}\\n```   \n",
       "3  ---\\nlibrary_name: transformers\\nlicense: apache-2.0\\nlicense_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking/blob/main/LICENSE\\npipeline_tag: text-generation\\n---\\n\\n# Qwen3-Next-80B-A3B-Thinking\\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\\n</a>\\n\\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). \\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. \\nWe call this next-generation foundation models **Qwen3-Next**.\\n\\n## Highlights\\n\\n**Qwen3-Next-80B-A3B** is the first installment in the Qwen3-Next series and features the following key enchancements:\\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling for ultra-long context length.\\n- **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. \\n- **Stability Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, and other stabilizing enhancements for robust pre-training and post-training.  \\n- **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\\n\\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\\n- Qwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\\n- Leveraging [GSPO](https://qwenlm.github.io/blog/gspo/), we have addressed the stability and efficiency challenges posed by the hybrid attention mechanism combined with a high-sparsity MoE architecture in RL training. \\n  Qwen3-Next-80B-A3B-Thinking demonstrates outstanding performance on complex reasoning tasks, not only **surpassing Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B-Thinking**, but also **outperforming the proprietary model Gemini-2.5-Flash-Thinking** across multiple benchmarks.\\n\\n![Qwen3-Next-80B-A3B-Thinking Benchmark Comparison](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/Qwen3-Next-80B-A3B-Thinking.001.jpeg)\\n\\nFor more details, please refer to our blog post [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list).\\n\\n## Model Overview\\n\\n> [!Note]\\n> **Qwen3-Next-80B-A3B-Thinking** supports only thinking mode. \\n> To enforce model thinking, the default chat template automatically includes `<think>`. \\n> Therefore, it is normal for the model's output to contain only `</think>` without an explicit opening `<think>` tag.\\n\\n> [!Note]\\n> **Qwen3-Next-80B-A3B-Thinking** may generate thinking content longer than its predecessor.\\n> We strongly recommend its use in highly complex reasoning tasks.\\n\\n\\n**Qwen3-Next-80B-A3B-Thinking** has the following features:\\n- Type: Causal Language Models\\n- Training Stage: Pretraining (15T tokens) & Post-training\\n- Number of Parameters: 80B in total and 3B activated\\n- Number of Paramaters (Non-Embedding): 79B\\n- Hidden Dimension: 2048\\n- Number of Layers: 48\\n  - Hybrid Layout: 12 \\* (3 \\* (Gated DeltaNet -> MoE) -> 1 \\* (Gated Attention -> MoE))\\n- Gated Attention:\\n  - Number of Attention Heads: 16 for Q and 2 for KV\\n  - Head Dimension: 256\\n  - Rotary Position Embedding Dimension: 64\\n- Gated DeltaNet:\\n  - Number of Linear Attention Heads: 32 for V and 16 for QK\\n  - Head Dimension: 128\\n- Mixture of Experts:\\n  - Number of Experts: 512\\n  - Number of Activated Experts: 10\\n  - Number of Shared Experts: 1\\n  - Expert Intermediate Dimension: 512\\n- Context Length: 262,144 natively and extensible up to 1,010,000 tokens\\n\\n<img src=\"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/model_architecture.png\" height=\"384px\" title=\"Qwen3-Next Model Architecture\" />\\n\\n\\n## Performance\\n\\n|  | Qwen3-30B-A3B-Thinking-2507 | Qwen3-32B Thinking | Qwen3-235B-A22B-Thinking-2507 | Gemini-2.5-Flash Thinking | Qwen3-Next-80B-A3B-Thinking |\\n|--- | --- | --- | --- | --- | --- |\\n| **Knowledge** | | | | |\\n| MMLU-Pro | 80.9 | 79.1 | **84.4** | 81.9 | 82.7 |\\n| MMLU-Redux | 91.4 | 90.9 | **93.8** | 92.1 | 92.5 |\\n| GPQA | 73.4 | 68.4 | 81.1 | **82.8** | 77.2 |\\n| SuperGPQA | 56.8 | 54.1 | **64.9** | 57.8 | 60.8 |\\n| **Reasoning** | | | | |\\n| AIME25 | 85.0 | 72.9 | **92.3** | 72.0 | 87.8 |\\n| HMMT25 | 71.4 | 51.5 | **83.9** | 64.2 | 73.9 |\\n| LiveBench 241125 | 76.8 | 74.9 | **78.4** | 74.3 | 76.6 |\\n| **Coding** | | | | |\\n| LiveCodeBench v6 (25.02-25.05) | 66.0 | 60.6 | **74.1** | 61.2 | 68.7 |\\n| CFEval | 2044 | 1986 | **2134** | 1995 | 2071 |\\n| OJBench | 25.1 | 24.1 | **32.5** | 23.5 | 29.7 |\\n| **Alignment** | | | | |\\n| IFEval | 88.9 | 85.0 | 87.8 | **89.8** | 88.9 |\\n| Arena-Hard v2* | 56.0 | 48.4 | **79.7** | 56.7 | 62.3 |\\n| WritingBench | 85.0 | 79.0 | **88.3** | 83.9 | 84.6 |\\n| **Agent** | | | | |\\n| BFCL-v3 |  **72.4** | 70.3 | 71.9 | 68.6 | 72.0 |\\n| TAU1-Retail | 67.8 | 52.8 | 67.8 | 65.2 | **69.6** |\\n| TAU1-Airline | 48.0 | 29.0 | 46.0 | **54.0** | 49.0 |\\n| TAU2-Retail | 58.8 | 49.7 | **71.9** | 66.7 | 67.8 |\\n| TAU2-Airline | 58.0 | 45.5 | 58.0 | 52.0 | **60.5** |\\n| TAU2-Telecom | 26.3 | 27.2 | **45.6** | 31.6 | 43.9 |\\n| **Multilingualism** | | | | |\\n| MultiIF | 76.4 | 73.0 | **80.6** | 74.4 | 77.8 |\\n| MMLU-ProX | 76.4 | 74.6 | **81.0** | 80.2 | 78.7 |\\n| INCLUDE | 74.4 | 73.7 | 81.0 | **83.9** | 78.9 |\\n| PolyMATH | 52.6 | 47.4 | **60.1** | 49.8 | 56.3 |\\n\\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\\n\\n## Quickstart\\n\\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face `transformers`. \\n\\n```shell\\npip install git+https://github.com/huggingface/transformers.git@main\\n```\\n\\nWith earlier versions, you will encounter the following error:\\n```\\nKeyError: 'qwen3_next'\\n```\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt},\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=32768,\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \\n\\n# parsing thinking content\\ntry:\\n    # rindex finding 151668 (</think>)\\n    index = len(output_ids) - output_ids[::-1].index(151668)\\nexcept ValueError:\\n    index = 0\\n\\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\\n\\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\\nprint(\"content:\", content)\\n```\\n\\n> [!Note]\\n> Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\\n\\n> [!Note]\\n> The efficiency or throughput improvement depends highly on the implementation.\\n> It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\\n\\n> [!Tip]\\n> Depending on the inference settings, you may observe better efficiency with [`flash-linear-attention`](https://github.com/fla-org/flash-linear-attention#installation) and [`causal-conv1d`](https://github.com/Dao-AILab/causal-conv1d).\\n> See the links for detailed instructions and requirements.\\n\\n## Deployment\\n\\nFor deployment, you can use the latest `sglang` or `vllm` to create an OpenAI-compatible API endpoint.\\n\\n### SGLang\\n\\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\\nSGLang could be used to launch a server with OpenAI-compatible API service. \\n\\n`sglang>=0.5.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'sglang[all]>=0.5.2'\\n```\\nSee [its documentation](https://docs.sglang.ai/get_started/install.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:30000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Thinking --port 30000 --tp-size 4 --context-length 262144 --reasoning-parser deepseek-r1 --mem-fraction-static 0.8\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Thinking --port 30000 --tp-size 4 --context-length 262144 --reasoning-parser deepseek-r1 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\\n```\\n\\n> [!Note]\\n> The default context length is 256K. \\n> If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. \\n> However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072.\\n\\nPlease also refer to SGLang's usage guide on [Qwen3-Next](https://docs.sglang.ai/basic_usage/qwen3.html).\\n\\n### vLLM\\n\\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\\nvLLM could be used to launch a server with OpenAI-compatible API service. \\n\\n`vllm>=0.10.2` is required for Qwen3-Next, which can be installed using:\\n```shell\\npip install 'vllm>=0.10.2'\\n```\\nSee [its documentation](https://docs.vllm.ai/en/stable/getting_started/installation/index.html) for more details.\\n\\nThe following command can be used to create an API endpoint at `http://localhost:8000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --reasoning-parser deepseek_r1\\n```\\n\\nThe following command is recommended for MTP with the rest settings the same as above:\\n```shell\\nvllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --reasoning-parser deepseek_r1 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\\n```\\n\\n> [!Note]\\n> The default context length is 256K. \\n> If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. \\n> However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\\n\\nPlease also refer to vLLM's usage guide on [Qwen3-Next](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html).\\n\\n\\n## Agentic Use\\n\\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\\n\\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\\n```python\\nfrom qwen_agent.agents import Assistant\\n\\n# Define LLM\\n# Using Alibaba Cloud Model Studio\\nllm_cfg = {\\n    'model': 'Qwen3-Next-80B-A3B-Thinking',\\n    'model_type': 'qwen_dashscope',\\n}\\n\\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example, \\n# `vllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --served-model-name Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144`.\\n#\\n# llm_cfg = {\\n#     'model': 'Qwen3-Next-80B-A3B-Thinking',\\n# \\n#     # Use a custom endpoint compatible with OpenAI API:\\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\\n#     'api_key': 'EMPTY',\\n#     'generate_cfg': {\\n#         'thought_in_content': True,\\n#     },\\n# }\\n\\n# Define Tools\\ntools = [\\n    {'mcpServers': {  # You can specify the MCP configuration file\\n            'time': {\\n                'command': 'uvx',\\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\\n            },\\n            \"fetch\": {\\n                \"command\": \"uvx\",\\n                \"args\": [\"mcp-server-fetch\"]\\n            }\\n        }\\n    },\\n  'code_interpreter',  # Built-in tools\\n]\\n\\n# Define Agent\\nbot = Assistant(llm=llm_cfg, function_list=tools)\\n\\n# Streaming generation\\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\\nfor responses in bot.run(messages=messages):\\n    pass\\nprint(responses)\\n```\\n\\n\\n## Processing Ultra-Long Texts\\n\\nQwen3-Next natively supports context lengths of up to 262,144 tokens. \\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. \\nWe have validated the model's performance on context lengths of up to 1 million tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\\n\\nYaRN is currently supported by several inference frameworks, e.g., `transformers`, `vllm` and `sglang`. \\nIn general, there are two approaches to enabling YaRN for supported frameworks:\\n\\n- Modifying the model files:\\n  In the `config.json` file, add the `rope_scaling` fields:\\n    ```json\\n    {\\n        ...,\\n        \"rope_scaling\": {\\n            \"rope_type\": \"yarn\",\\n            \"factor\": 4.0,\\n            \"original_max_position_embeddings\": 262144\\n        }\\n    }\\n    ```\\n\\n- Passing command line arguments:\\n\\n  For `vllm`, you can use\\n    ```shell\\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000  \\n    ```\\n\\n  For `sglang`, you can use\\n    ```shell\\n    SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\\n    ```\\n\\n> [!NOTE]\\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \\n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set `factor` as 2.0. \\n\\n## Best Practices\\n\\nTo achieve optimal performance, we recommend the following settings:\\n\\n1. **Sampling Parameters**:\\n   - We suggest using `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`.\\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\\n\\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\\n\\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\\n\\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\\n\\n### Citation\\n\\nIf you find our work helpful, feel free to give us a cite.\\n\\n```\\n@misc{qwen3technicalreport,\\n      title={Qwen3 Technical Report}, \\n      author={Qwen Team},\\n      year={2025},\\n      eprint={2505.09388},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2505.09388}, \\n}\\n\\n@article{qwen2.5-1m,\\n      title={Qwen2.5-1M Technical Report}, \\n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\\n      journal={arXiv preprint arXiv:2501.15383},\\n      year={2025}\\n}\\n```   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ---\\nlicense: gemma\\npipeline_tag: sentence-similarity\\nlibrary_name: sentence-transformers\\ntags:\\n- sentence-transformers\\n- sentence-similarity\\n- feature-extraction\\n- text-embeddings-inference\\nextra_gated_heading: Access EmbeddingGemma on Hugging Face\\nextra_gated_prompt: To access EmbeddingGemma on Hugging Face, you‚Äôre required to review and\\n  agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging\\n  Face and click below. Requests are processed immediately.\\nextra_gated_button_content: Acknowledge license\\n---\\n\\n# EmbeddingGemma model card\\n\\n**Model Page**: [EmbeddingGemma](https://ai.google.dev/gemma/docs/embeddinggemma)\\n\\n**Resources and Technical Documentation**:\\n\\n*   [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\\n*   [EmbeddingGemma on Kaggle](https://www.kaggle.com/models/google/embeddinggemma/)\\n*   [EmbeddingGemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/embeddinggemma)\\n\\n**Terms of Use**: [Terms](https://ai.google.dev/gemma/terms)\\n\\n**Authors**: Google DeepMind\\n\\n## Model Information\\n\\n### Description\\n\\nEmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.\\n\\nThe small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone.\\n\\n### Inputs and outputs\\n\\n-   **Input:**\\n    -   Text string, such as a question, a prompt, or a document to be embedded\\n    -   Maximum input context length of 2048 tokens\\n\\n-   **Output:**\\n    -   Numerical vector representations of input text data\\n    -   Output embedding dimension size of 768, with smaller options available (512, 256, or 128) via Matryoshka Representation Learning (MRL). MRL allows users to truncate the output embedding of size 768 to their desired size and then re-normalize for efficient and accurate representation.\\n\\n### Usage\\n\\nThese model weights are designed to be used with [Sentence Transformers](https://www.SBERT.net), using the [Gemma 3](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3) implementation from [Hugging Face Transformers](https://huggingface.co/docs/transformers/en/index) as the backbone.\\n\\nFirst install the Sentence Transformers library:\\n\\n```bash\\npip install -U sentence-transformers\\n```\\n\\nThen you can load this model and run inference.\\n\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\n# Download from the ü§ó Hub\\nmodel = SentenceTransformer(\"google/embeddinggemma-300m\")\\n\\n# Run inference with queries and documents\\nquery = \"Which planet is known as the Red Planet?\"\\ndocuments = [\\n    \"Venus is often called Earth's twin because of its similar size and proximity.\",\\n    \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\\n    \"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\\n    \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\\n]\\nquery_embeddings = model.encode_query(query)\\ndocument_embeddings = model.encode_document(documents)\\nprint(query_embeddings.shape, document_embeddings.shape)\\n# (768,) (4, 768)\\n\\n# Compute similarities to determine a ranking\\nsimilarities = model.similarity(query_embeddings, document_embeddings)\\nprint(similarities)\\n# tensor([[0.3011, 0.6359, 0.4930, 0.4889]])\\n```\\n\\n**NOTE**: EmbeddingGemma activations do not support `float16`. Please use `float32` or `bfloat16` as appropriate for your hardware.\\n\\n## Model Data\\n\\n### Training Dataset\\n\\nThis model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components:\\n\\n-   **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages.\\n-   **Code and Technical Documents**: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.\\n-   **Synthetic and Task-Specific Data**: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications.\\n\\nThe combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats.\\n\\n### Data Preprocessing\\n\\nHere are the key data cleaning and filtering methods applied to the training data:\\n\\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content.\\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.\\n-   Additional methods: Filtering based on content quality and safety in line with [our policies](https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf).\\n\\n## Model Development\\n\\n### Hardware\\n\\nEmbeddingGemma was trained using the latest generation of [Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv5e), for more details refer to the [Gemma 3 model card](https://ai.google.dev/gemma/docs/core/model_card_3).\\n\\n### Software\\n\\nTraining was done using [JAX](https://github.com/jax-ml/jax) and [ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/). For more details refer to the [Gemma 3 model card](https://ai.google.dev/gemma/docs/core/model_card_3).\\n\\n## Evaluation\\n\\n### Benchmark Results\\n\\nThe model was evaluated against a large collection of different datasets and metrics to cover different aspects of text understanding.\\n\\n#### Full Precision Checkpoint\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th colspan=\"3\"><strong>MTEB (Multilingual, v2)</strong></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><strong>Dimensionality</strong></td>\\n      <td><strong>Mean (Task)</strong></td>\\n      <td><strong>Mean (TaskType)</strong></td>\\n    </tr>\\n    <tr>\\n      <td>768d</td>\\n      <td>61.15</td>\\n      <td>54.31</td>\\n    </tr>\\n    <tr>\\n      <td>512d</td>\\n      <td>60.71</td>\\n      <td>53.89</td>\\n    </tr>\\n    <tr>\\n      <td>256d</td>\\n      <td>59.68</td>\\n      <td>53.01</td>\\n    </tr>\\n    <tr>\\n      <td>128d</td>\\n      <td>58.23</td>\\n      <td>51.77</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th colspan=\"3\"><strong>MTEB (English, v2)</strong></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><strong>Dimensionality</strong></td>\\n      <td><strong>Mean (Task)</strong></td>\\n      <td><strong>Mean (TaskType)</strong></td>\\n    </tr>\\n    <tr>\\n      <td>768d</td>\\n      <td>68.36</td>\\n      <td>64.15</td>\\n    </tr>\\n    <tr>\\n      <td>512d</td>\\n      <td>67.80</td>\\n      <td>63.59</td>\\n    </tr>\\n    <tr>\\n      <td>256d</td>\\n      <td>66.89</td>\\n      <td>62.94</td>\\n    </tr>\\n    <tr>\\n      <td>128d</td>\\n      <td>65.09</td>\\n      <td>61.56</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th colspan=\"3\"><strong>MTEB (Code, v1)</strong></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><strong>Dimensionality</strong></td>\\n      <td><strong>Mean (Task)</strong></td>\\n      <td><strong>Mean (TaskType)</strong></td>\\n    </tr>\\n    <tr>\\n      <td>768d</td>\\n      <td>68.76</td>\\n      <td>68.76</td>\\n    </tr>\\n    <tr>\\n      <td>512d</td>\\n      <td>68.48</td>\\n      <td>68.48</td>\\n    </tr>\\n    <tr>\\n      <td>256d</td>\\n      <td>66.74</td>\\n      <td>66.74</td>\\n    </tr>\\n    <tr>\\n      <td>128d</td>\\n      <td>62.96</td>\\n      <td>62.96</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n#### QAT Checkpoints\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th colspan=\"3\"><strong>MTEB (Multilingual, v2)</strong></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><strong>Quant config (dimensionality)</strong></td>\\n      <td><strong>Mean (Task)</strong></td>\\n      <td><strong>Mean (TaskType)</strong></td>\\n    </tr>\\n    <tr>\\n      <td>Q4_0 (768d)</td>\\n      <td>60.62</td>\\n      <td>53.61</td>\\n    </tr>\\n    <tr>\\n      <td>Q8_0 (768d)</td>\\n      <td>60.93</td>\\n      <td>53.95</td>\\n    </tr>\\n    <tr>\\n      <td>Mixed Precision* (768d)</td>\\n      <td>60.69</td>\\n      <td>53.82</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th colspan=\"3\"><strong>MTEB (English, v2)</strong></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><strong>Quant config (dimensionality)</strong></td>\\n      <td><strong>Mean (Task)</strong></td>\\n      <td><strong>Mean (TaskType)</strong></td>\\n    </tr>\\n    <tr>\\n      <td>Q4_0 (768d)</td>\\n      <td>67.91</td>\\n      <td>63.64</td>\\n    </tr>\\n    <tr>\\n      <td>Q8_0 (768d)</td>\\n      <td>68.13</td>\\n      <td>63.85</td>\\n    </tr>\\n    <tr>\\n      <td>Mixed Precision* (768d)</td>\\n      <td>67.95</td>\\n      <td>63.83</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th colspan=\"3\"><strong>MTEB (Code, v1)</strong></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><strong>Quant config (dimensionality)</strong></td>\\n      <td><strong>Mean (Task)</strong></td>\\n      <td><strong>Mean (TaskType)</strong></td>\\n    </tr>\\n    <tr>\\n      <td>Q4_0 (768d)</td>\\n      <td>67.99</td>\\n      <td>67.99</td>\\n    </tr>\\n    <tr>\\n      <td>Q8_0 (768d)</td>\\n      <td>68.70</td>\\n      <td>68.70</td>\\n    </tr>\\n    <tr>\\n      <td>Mixed Precision* (768d)</td>\\n      <td>68.03</td>\\n      <td>68.03</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\nNote: QAT models are evaluated after quantization\\n\\n\\* Mixed Precision refers to per-channel quantization with int4 for embeddings, feedforward, and projection layers, and int8 for attention (e4_a8_f4_p4).\\n\\n### Prompt Instructions\\n\\nEmbeddingGemma can generate optimized embeddings for various use cases‚Äîsuch as document retrieval, question answering, and fact verification‚Äîor for specific input types‚Äîeither a query or a document‚Äîusing prompts that are prepended to the input strings.\\nQuery prompts follow the form `task: {task description} | query: ` where the task description varies by the use case, with the default task description being `search result`. Document-style prompts follow the form `title: {title | \"none\"} | text: ` where the title is either `none` (the default) or the actual title of the document. Note that providing a title, if available, will improve model performance for document prompts but may require manual formatting.\\n\\nUse the following prompts based on your use case and input data type. These may already be available in the EmbeddingGemma configuration in your modeling framework of choice.\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th><br>\\n<strong>Use Case (task type enum)</strong></th>\\n      <th><br>\\n<strong>Descriptions</strong></th>\\n      <th><br>\\n<strong>Recommended Prompt</strong></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><br>\\nRetrieval (Query)</td>\\n      <td rowspan=\"4\"><br>\\nUsed to generate embeddings that are optimized for document search or information retrieval</td>\\n      <td><br>\\ntask: search result | query: {content}</td>\\n    </tr>\\n    <tr>\\n      <td><br>\\nRetrieval (Document)</td>\\n      <td><br>\\ntitle: {title | \"none\"} | text: {content}</td>\\n    </tr>\\n    <tr>\\n      <td><br>\\nQuestion Answering</td>\\n      <td><br>\\ntask: question answering | query: {content}</td>\\n    </tr>\\n    <tr>\\n      <td><br>\\nFact Verification</td>\\n      <td><br>\\ntask: fact checking | query: {content}</td>\\n    </tr>\\n    <tr>\\n      <td><br>\\nClassification</td>\\n      <td><br>\\nUsed to generate embeddings that are optimized to classify texts according to preset labels</td>\\n      <td><br>\\ntask: classification | query: {content}</td>\\n    </tr>\\n    <tr>\\n      <td><br>\\nClustering</td>\\n      <td><br>\\nUsed to generate embeddings that are optimized to cluster texts based on their similarities</td>\\n      <td><br>\\ntask: clustering | query: {content}</td>\\n    </tr>\\n    <tr>\\n      <td><br>\\nSemantic Similarity</td>\\n      <td><br>\\nUsed to generate embeddings that are optimized to assess text similarity. This is not intended for retrieval use cases.</td>\\n      <td><br>\\ntask: sentence similarity | query: {content}</td>\\n    </tr>\\n    <tr>\\n      <td><br>\\nCode Retrieval</td>\\n      <td><br>\\nUsed to retrieve a code block based on a natural language query, such as <em>sort an array</em> or <em>reverse a linked list</em>. Embeddings of the code blocks are computed using retrieval_document.</td>\\n      <td><br>\\ntask: code retrieval | query: {content}</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n## Usage and Limitations\\n\\nThese models have certain limitations that users should be aware of.\\n\\n### Intended Usage\\n\\nOpen embedding models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.\\n\\n-   **Semantic Similarity**: Embeddings optimized to assess text similarity, such as recommendation systems and duplicate detection\\n-   **Classification**: Embeddings optimized to classify texts according to preset labels, such as sentiment analysis and spam detection\\n-  **Clustering**: Embeddings optimized to cluster texts based on their similarities, such as document organization, market research, and anomaly detection\\n-   **Retrieval**\\n    -   **Document**: Embeddings optimized for document search, such as indexing articles, books, or web pages for search\\n    -   **Query**: Embeddings optimized for general search queries, such as custom search\\n    -   **Code Query**: Embeddings optimized for retrieval of code blocks based on natural language queries, such as code suggestions and search\\n\\n-   **Question Answering**: Embeddings for questions in a question-answering system, optimized for finding documents that answer the question, such as chatbox.\\n-   **Fact Verification**: Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement, such as automated fact-checking systems.\\n\\n### Limitations\\n\\n-   Training Data\\n    -   The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.\\n    -   The scope of the training dataset determines the subject areas the model can handle effectively.\\n\\n-   Language Ambiguity and Nuance\\n    -   Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language.\\n\\n### Ethical Considerations and Risks\\n\\nRisks identified and mitigations:\\n\\n-   **Perpetuation of biases**: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases.\\n-   **Misuse for malicious purposes**: Technical limitations and developer and end-user education can help mitigate against malicious applications of embeddings. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in the [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\\n-   **Privacy violations**: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\\n\\n### Benefits\\n\\nAt the time of release, this family of models provides high-performance open embedding model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown superior performance to other, comparably-sized open model alternatives.   \n",
       "\n",
       "                   createdAt   type y_multi_lab  \\\n",
       "0  2025-09-08T12:44:15+00:00  model        [19]   \n",
       "1  2025-09-08T14:18:31+00:00  model         [0]   \n",
       "2  2025-09-09T15:40:56+00:00  model         [0]   \n",
       "3  2025-09-09T15:45:31+00:00  model         [0]   \n",
       "4  2025-07-17T19:53:55+00:00  model     [24, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             relationships  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "2  model_finetune_model:unsloth/Qwen3-Next-80B-A3B-Instruct, tiny-random/qwen3-next-moe, kikekewl/Qwen3-Next-80B-A3B-mlx-bf16, model_quantized_model:cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit, unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit, mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit, Intel/Qwen3-Next-80B-A3B-Instruct-int4-mixed-AutoRound, nightmedia/Qwen3-Next-80B-A3B-Instruct-q2-mlx, nightmedia/Qwen3-Next-80B-A3B-Instruct-mxfp4-mlx, DevQuasar/Qwen.Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic, TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8, nightmedia/Qwen3-Next-80B-A3B-Instruct-qx86-hi-mlx, mlx-community/Qwen3-Next-80B-A3B-Instruct-5bit, mlx-community/Qwen3-Next-80B-A3B-Instruct-6bit, mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit, TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         model_finetune_model:kikekewl/Qwen3-Next-80B-A3B-Thinking-mlx-bf16, model_quantized_model:Intel/Qwen3-Next-80B-A3B-Thinking-int4-mixed-AutoRound, cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit, llllwxxx/Qwen3-Next-80B-A3B-Thinking-FP8-Dynamic, mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit   \n",
       "4                                                                                                                                                                                                                                                                                                                           model_finetune_model:sigridjineth/colbert-ko-embeddinggemma-300m, sentence-transformers/embeddinggemma-300m-medical, yasserrmd/geo-gemma-300m-emb, unsloth/embeddinggemma-300m, Omartificial-Intelligence-Space/AraGemma-Embedding-300m, Saidakmal/uz_embeddinggemma-300m, yasserrmd/pharma-gemma-300m-emb, model_quantized_model:onnx-community/embeddinggemma-300m-ONNX, unsloth/embeddinggemma-300m-GGUF, ggml-org/embeddinggemma-300M-GGUF, SandLogicTechnologies/EmbeddingGemma-300m-GGUF   \n",
       "\n",
       "                                                                                                                                                                    y  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pd.set_option('display.max_columns', 200)\n",
    "# pd.set_option('display.max_colwidth', 200)\n",
    "# pd.set_option('display.width', 200)\n",
    "\n",
    "display(nodes_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27b10a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Files loaded successfully!\n",
      "edges_df shape: (303296, 4)\n",
      "old_to_new_idx size: 107694\n",
      "task_to_idx: {'text-generation': 0, 'question-answering': 1, 'text-to-video': 2, 'image-to-video': 3, 'image-to-3d': 4, 'robotics': 5, 'translation': 6, 'feature-extraction': 7, 'text-to-3d': 8, 'text-to-speech': 9, 'automatic-speech-recognition': 10, 'image-classification': 11, 'table-question-answering': 12, 'fill-mask': 13, 'multiple-choice': 14, 'visual-question-answering': 15, 'summarization': 16, 'image-to-text': 17, 'image-feature-extraction': 18, 'text-to-image': 19, 'text-to-audio': 20, 'reinforcement-learning': 21, 'image-text-to-text': 22, 'text-classification': 23, 'sentence-similarity': 24, 'zero-shot-classification': 25, 'text-retrieval': 26, 'token-classification': 27, 'object-detection': 28, 'audio-classification': 29, 'image-segmentation': 30, 'time-series-forecasting': 31, 'video-classification': 32, 'zero-shot-image-classification': 33, 'any-to-any': 34, 'image-to-image': 35, 'depth-estimation': 36, 'tabular-classification': 37, 'tabular-regression': 38, 'table-to-text': 39, 'video-text-to-text': 40, 'audio-to-audio': 41, 'voice-activity-detection': 42, 'audio-text-to-text': 43, 'document-question-answering': 44, 'visual-document-retrieval': 45, 'text-ranking': 46, 'graph-ml': 47, 'tabular-to-text': 48, 'unconditional-image-generation': 49, 'mask-generation': 50, 'keypoint-detection': 51, 'zero-shot-object-detection': 52, 'video-to-video': 53}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# --- Load Edges ---\n",
    "with open(\"edges_df.pkl\", \"rb\") as f:\n",
    "    edges_df = pickle.load(f)\n",
    "\n",
    "# --- Load Mapping Dictionaries ---\n",
    "with open(\"old_to_new_idx.json\", \"r\") as f:\n",
    "    old_to_new_idx = json.load(f)\n",
    "\n",
    "with open(\"task_to_idx.json\", \"r\") as f:\n",
    "    task_to_idx = json.load(f)\n",
    "\n",
    "print(\"‚úÖ Files loaded successfully!\")\n",
    "print(\"edges_df shape:\", edges_df.shape)\n",
    "print(\"old_to_new_idx size:\", len(old_to_new_idx))\n",
    "print(\"task_to_idx:\", task_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fbbc430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_node</th>\n",
       "      <th>dest_node</th>\n",
       "      <th>edge_type</th>\n",
       "      <th>edge_attr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>rockerBOO/flux.1-dev-SRPO</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-Refine-Quantized-v1.0</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>befox/SRPO-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>unsloth/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            source_node  \\\n",
       "match                                     \n",
       "0                          tencent/SRPO   \n",
       "1                          tencent/SRPO   \n",
       "1                          tencent/SRPO   \n",
       "1                          tencent/SRPO   \n",
       "0      baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "1      baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "1      baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "1      baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "1      baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "1      baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "\n",
       "                                           dest_node               edge_type  \\\n",
       "match                                                                          \n",
       "0                          rockerBOO/flux.1-dev-SRPO    model_finetune_model   \n",
       "1               wikeeyang/SRPO-Refine-Quantized-v1.0   model_quantized_model   \n",
       "1                                    befox/SRPO-GGUF   model_quantized_model   \n",
       "1                         wikeeyang/SRPO-for-ComfyUI   model_quantized_model   \n",
       "0                 unsloth/ERNIE-4.5-21B-A3B-Thinking    model_finetune_model   \n",
       "1            unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF   model_quantized_model   \n",
       "1      gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF   model_quantized_model   \n",
       "1        cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit   model_quantized_model   \n",
       "1        cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit   model_quantized_model   \n",
       "1       mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF   model_quantized_model   \n",
       "\n",
       "       edge_attr  \n",
       "match             \n",
       "0              0  \n",
       "1              3  \n",
       "1              3  \n",
       "1              3  \n",
       "0              0  \n",
       "1              3  \n",
       "1              3  \n",
       "1              3  \n",
       "1              3  \n",
       "1              3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119454d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_node', 'dest_node', 'edge_type', 'edge_attr'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2221db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned edge_type values:\n",
      "['model_finetune_model' 'model_quantized_model' 'model_adapter_model'\n",
      " 'model_trainedOrFineTunedOn_dataset' 'model_merge_model']\n"
     ]
    }
   ],
   "source": [
    "# remove spaces in edge_type\n",
    "edges_df[\"edge_type\"] = (\n",
    "    edges_df[\"edge_type\"]\n",
    "    .astype(str)\n",
    "    .str.strip()                   # remove leading/trailing spaces\n",
    "    .str.replace(r\"\\s+\", \"_\", regex=True)  # collapse internal spaces to underscores if any\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Cleaned edge_type values:\")\n",
    "print(edges_df[\"edge_type\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173f36ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Deduplicated edges_df now has 299702 rows (in place).\n"
     ]
    }
   ],
   "source": [
    "# dedup edges\n",
    "\n",
    "SRC_COL = \"source_node\"            # or \"source_node\"\n",
    "DST_COL = \"dest_node\"            # or \"dest_node\"\n",
    "EDGE_TYPE_COL = \"edge_type\"\n",
    "EDGE_ATTR_COL = \"edge_attr\"\n",
    "\n",
    "# Canonicalize undirected pair so (u,v) == (v,u)\n",
    "edges_df[\"canon_u\"] = edges_df[[SRC_COL, DST_COL]].min(axis=1)\n",
    "edges_df[\"canon_v\"] = edges_df[[SRC_COL, DST_COL]].max(axis=1)\n",
    "\n",
    "# Drop duplicates in-place using the canonical keys + attributes\n",
    "edges_df.drop_duplicates(\n",
    "    subset=[\"canon_u\", \"canon_v\", EDGE_TYPE_COL],\n",
    "    keep=\"first\",\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Drop helper columns now that dedup is done\n",
    "edges_df.drop(columns=[\"canon_u\", \"canon_v\"], inplace=True)\n",
    "\n",
    "# Reset index to keep things tidy\n",
    "edges_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Deduplicated edges_df now has {len(edges_df)} rows (in place).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa70f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added columns 'source_type' and 'dest_type'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_node</th>\n",
       "      <th>dest_node</th>\n",
       "      <th>edge_type</th>\n",
       "      <th>edge_attr</th>\n",
       "      <th>source_type</th>\n",
       "      <th>dest_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>rockerBOO/flux.1-dev-SRPO</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-Refine-Quantized-v1.0</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>befox/SRPO-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>unsloth/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        source_node  \\\n",
       "0                      tencent/SRPO   \n",
       "1                      tencent/SRPO   \n",
       "2                      tencent/SRPO   \n",
       "3                      tencent/SRPO   \n",
       "4  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "5  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "6  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "7  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "8  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "9  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "\n",
       "                                       dest_node              edge_type  \\\n",
       "0                      rockerBOO/flux.1-dev-SRPO   model_finetune_model   \n",
       "1           wikeeyang/SRPO-Refine-Quantized-v1.0  model_quantized_model   \n",
       "2                                befox/SRPO-GGUF  model_quantized_model   \n",
       "3                     wikeeyang/SRPO-for-ComfyUI  model_quantized_model   \n",
       "4             unsloth/ERNIE-4.5-21B-A3B-Thinking   model_finetune_model   \n",
       "5        unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF  model_quantized_model   \n",
       "6  gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF  model_quantized_model   \n",
       "7    cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit  model_quantized_model   \n",
       "8    cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit  model_quantized_model   \n",
       "9   mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF  model_quantized_model   \n",
       "\n",
       "   edge_attr source_type dest_type  \n",
       "0          0       model     model  \n",
       "1          3       model     model  \n",
       "2          3       model     model  \n",
       "3          3       model     model  \n",
       "4          0       model     model  \n",
       "5          3       model     model  \n",
       "6          3       model     model  \n",
       "7          3       model     model  \n",
       "8          3       model     model  \n",
       "9          3       model     model  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split edge_type into source_type and dest_type\n",
    "edges_df[\"source_type\"] = edges_df[\"edge_type\"].str.split(\"_\").str[0]\n",
    "edges_df[\"dest_type\"] = edges_df[\"edge_type\"].str.split(\"_\").str[-1]\n",
    "\n",
    "print(\"‚úÖ Added columns 'source_type' and 'dest_type'\")\n",
    "display(edges_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc19d686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 0),\n",
       " ('1', 1),\n",
       " ('2', 2),\n",
       " ('3', 3),\n",
       " ('4', 4),\n",
       " ('5', 5),\n",
       " ('6', 6),\n",
       " ('7', 7),\n",
       " ('8', 8),\n",
       " ('10', 9),\n",
       " ('12', 10),\n",
       " ('13', 11),\n",
       " ('14', 12),\n",
       " ('15', 13),\n",
       " ('16', 14),\n",
       " ('18', 15),\n",
       " ('19', 16),\n",
       " ('20', 17),\n",
       " ('22', 18),\n",
       " ('23', 19),\n",
       " ('24', 20),\n",
       " ('25', 21),\n",
       " ('26', 22),\n",
       " ('27', 23),\n",
       " ('28', 24),\n",
       " ('30', 25),\n",
       " ('31', 26),\n",
       " ('33', 27),\n",
       " ('34', 28),\n",
       " ('35', 29)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(old_to_new_idx.items())[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b588aeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'description', 'createdAt', 'type', 'y_multi_lab',\n",
       "       'relationships', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e15ff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded encoder: SentenceTransformer(model_name=/home/hice1/cxu371/scratch/huggingface_cache/hub/models--BAAI--bge-base-en-v1.5/snapshots/a5beb1e3e68b9ab74eb54cfd186867f64f240e1a)\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torch_geometric.llm.models import SentenceTransformer\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# # ---- setup ----\n",
    "# os.environ[\"HF_HOME\"] = \"/home/hice1/cxu371/scratch/hf_cache\"\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/cxu371/scratch/hf_cache\"\n",
    "\n",
    "# # authenticate to HF\n",
    "# login(token=\"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\")  # or directly pass the string token\n",
    "\n",
    "# # ---- load model ----\n",
    "# encoder = SentenceTransformer(\"BAAI/bge-base-en-v1.5\").to(\"cuda\")\n",
    "# print(\"Loaded encoder:\", encoder)\n",
    "\n",
    "# loading from my snapshot\n",
    "import os\n",
    "from torch_geometric.llm.models import SentenceTransformer\n",
    "\n",
    "# Optional: make sure caches point to scratch\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/cxu371/scratch/huggingface_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/hice1/cxu371/scratch/huggingface_cache\"\n",
    "\n",
    "# Directly point to the snapshot directory\n",
    "local_model_path = \"/home/hice1/cxu371/scratch/huggingface_cache/hub/models--BAAI--bge-base-en-v1.5/snapshots/a5beb1e3e68b9ab74eb54cfd186867f64f240e1a\"\n",
    "\n",
    "encoder = SentenceTransformer(local_model_path).to(\"cuda\")\n",
    "print(\"‚úÖ Loaded encoder:\", encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55adb157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(model_name=/home/hice1/cxu371/scratch/huggingface_cache/hub/models--BAAI--bge-base-en-v1.5/snapshots/a5beb1e3e68b9ab74eb54cfd186867f64f240e1a)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "532670f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_node</th>\n",
       "      <th>dest_node</th>\n",
       "      <th>edge_type</th>\n",
       "      <th>edge_attr</th>\n",
       "      <th>source_type</th>\n",
       "      <th>dest_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>rockerBOO/flux.1-dev-SRPO</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-Refine-Quantized-v1.0</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_node                             dest_node              edge_type  \\\n",
       "0  tencent/SRPO             rockerBOO/flux.1-dev-SRPO   model_finetune_model   \n",
       "1  tencent/SRPO  wikeeyang/SRPO-Refine-Quantized-v1.0  model_quantized_model   \n",
       "\n",
       "   edge_attr source_type dest_type  \n",
       "0          0       model     model  \n",
       "1          3       model     model  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d501fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'description', 'createdAt', 'type', 'y_multi_lab',\n",
       "       'relationships', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10a68768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>type</th>\n",
       "      <th>y_multi_lab</th>\n",
       "      <th>relationships</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>model</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>model</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0                      tencent/SRPO   \n",
       "1  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "1  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "\n",
       "                   createdAt   type y_multi_lab  \\\n",
       "0  2025-09-08T12:44:15+00:00  model        [19]   \n",
       "1  2025-09-08T14:18:31+00:00  model         [0]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                               relationships  \\\n",
       "0                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "1  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "\n",
       "                                                                                                                                                                    y  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3c8196f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299702, 6)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7b2b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined dataframe shape: (299702, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_node</th>\n",
       "      <th>dest_node</th>\n",
       "      <th>edge_type</th>\n",
       "      <th>edge_attr</th>\n",
       "      <th>source_type</th>\n",
       "      <th>dest_type</th>\n",
       "      <th>source_description</th>\n",
       "      <th>dest_description</th>\n",
       "      <th>source_createdAt</th>\n",
       "      <th>source_y_multi_lab</th>\n",
       "      <th>source_relationships</th>\n",
       "      <th>source_y</th>\n",
       "      <th>dest_createdAt</th>\n",
       "      <th>dest_y_multi_lab</th>\n",
       "      <th>dest_relationships</th>\n",
       "      <th>dest_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>rockerBOO/flux.1-dev-SRPO</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>---\\nbase_model:\\n- tencent/SRPO\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n## bf16 and (remaking FP8 version) versions of SRPO from Tencent\\n\\n&lt;div align=\"center\" style=\"font-family: charter;\"&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n&lt;/div&gt;\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Quick Started\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n\\n#### Inference\\nReplace the `diffusion_pytorch_model.safetensors` of FLUX\\n```python\\nfrom diffusers import FluxPipeline\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=infer_step,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-10T21:11:28+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:Alissonerdx/flux.1-dev-SRPO-LoRas</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-Refine-Quantized-v1.0</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\nlanguage:\\n- en\\nbase_model:\\n- tencent/SRPO\\n---\\n===================================================================================\\n\\nÊú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ Á≤æË∞É Âíå 8bit/4bit (fp8_e4m3fn/Q8_0/Q4_1) ÈáèÂåñÁâàÊú¨Ôºå‰∏ªË¶ÅÊèêÂçáÂá∫ÂõæÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÊ®°ÂûãÁöÑÂÖºÂÆπÊÄß(Á¨¨‰∏ÄÂº†ÂõæÁâá‰∏≠ÁöÑ SRPO-fp8 ÈáèÂåñÁîüÊàêÁöÑÂõæÁâáÔºåÊòæÂæóÁâπÂà´Ê®°Á≥äÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÈááÁî® ComfyUI Ê®°ÂûãÂä†ËΩΩÂπ∂Áõ¥Êé•ÈáèÂåñÁöÑÊñπÂºèÈÄ†ÊàêÔºåÂπ∂ÈùûÊ®°Âûã fp8 Á≤æÂ∫¶‰∏ãÁöÑÂÆûÈôÖË°®Áé∞ÔºåÂÆûÈôÖË°®Áé∞ËØ∑ÂèÇÈòÖÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºå‰∏∫ÈÅøÂÖç‰ΩøÁî®ËÄÖËØØËß£ÔºåÁâπÊèê‰æõÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºåÊ®°ÂûãÂú®‰∏çÂêåÁ≤æÂ∫¶‰∏ãÁöÑË°®Áé∞ÊòØÊ≠£Â∏∏ÁöÑ)„ÄÇ\\n\\nThis model is the refine and quantized version of the model: https://huggingface.co/tencent/SRPO, it improve the clarity of the generated images and the compatibility of the models.\\n(In below image, the SRPO-fp8 means load and quantized directly by ComfyUI diffusion model loader nodes)\\n&lt;p align=\"center\"&gt;\\n    &lt;img src=\"Compare.jpg\" width=\"1200\"/&gt;\\n&lt;p&gt;\\n\\n&lt;u&gt;Compare SRPO offical and R&amp;Q v1.0 in the same quantized accuracy:&lt;/u&gt;\\n\\n&lt;p align=\"center\"&gt;\\n    &lt;img src=\"Compare-02.jpg\" width=\"1200\"/&gt;\\n&lt;p&gt;\\n\\n## Example workflow: Please refer to workflow.png\\n\\n## License Agreement\\n\\nPlease fall under SRPO license refer license.txt file and refer to the FLUX.1 [dev] Non-Commercial License. \\n\\nAlso: https://civitai.com/models/1953067\\n\\n‰ª•‰∏ãÈÉ®ÂàÜÂºïÁî®Ëá™ÂéüÊ®°ÂûãËØ¥ÊòéÂÜÖÂÆπÔºö\\n\\n===================================================================================\\n\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-13T05:29:39+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>befox/SRPO-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\nlanguage:\\n- en\\nbase_model:\\n- tencent/SRPO\\n---\\n===================================================================================\\n\\nÊú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ ËΩ¨Êç¢ Âíå 8bit/4bit (fp8_e4m3fn/Q8_0/Q4_1) ÈáèÂåñÁâàÊú¨Ôºå‰ª•ÈÄÇÈÖç ComfyUI Áî®Êà∑ÁéØÂ¢ÉÊ≠£Â∏∏Âä†ËΩΩÂíåÂá∫ÂõæÔºå‰øùÊåÅÂéüÊ®°ÂûãÊ≠£Â∏∏ÁöÑÂá∫ÂõæÊïàÊûú„ÄÇ\\n\\nThis model is the converted and quantized version of the model: https://huggingface.co/tencent/SRPO, To adapt the ComfyUI environment for normal loading and output of images, maintaining the original model's normal effects.\\n\\n&lt;u&gt; For bf16 version, Pls download it from: https://www.modelscope.cn/models/wikeeyang/SRPO-for-ComfyUI &lt;/u&gt;\\n\\n&lt;p align=\"center\"&gt;\\n    &lt;img src=\"example.jpg\" width=\"1200\"/&gt;\\n&lt;p&gt;\\n\\n## License Agreement\\n\\nPlease fall under SRPO license refer license.txt file and refer to the FLUX.1 [dev] Non-Commercial License. \\n\\n\\n‰ª•‰∏ãÈÉ®ÂàÜÂºïÁî®Ëá™ÂéüÊ®°ÂûãËØ¥ÊòéÂÜÖÂÆπÔºö\\n\\n===================================================================================\\n\\n\\n&lt;div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù&gt;\\n&lt;h1 align=\"center\"&gt;Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &lt;/h1&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;a href='https://arxiv.org/abs/2509.06942'&gt;&lt;img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'&gt;&lt;/a&gt;  &amp;nbsp;\\n  &lt;a href='https://github.com/Tencent-Hunyuan/SRPO'&gt;&lt;img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&amp;logo=github&amp;logoColor=whitee'&gt;&lt;/a&gt; &amp;nbsp; \\n  &lt;a href='https://tencent.github.io/srpo-project-page/'&gt;&lt;img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'&gt;&lt;/a&gt; &amp;nbsp;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  Xiangwei Shen&lt;sup&gt;1,2*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&amp;hl=zh-CN\" target=\"_blank\"&gt;&lt;b&gt;Zhimin Li&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;,\\n  &lt;a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"&gt;&lt;b&gt;Zhantao Yang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, \\n  &lt;a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"&gt;&lt;b&gt;Shiyi Zhang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,\\n  Yingfang Zhang&lt;sup&gt;1&lt;/sup&gt;,\\n  Donghao Li&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;br&gt;\\n  &lt;a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&amp;hl=en\" target=\"_blank\"&gt;&lt;b&gt;Chunyu Wang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"&gt;&lt;b&gt;Qinglin Lu&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,\\n  &lt;a href=\"https://andytang15.github.io\" target=\"_blank\"&gt;&lt;b&gt;Yansong Tang&lt;/b&gt;&lt;/a&gt;&lt;sup&gt;3,‚úù&lt;/sup&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\"&gt;\\n  &lt;sup&gt;1&lt;/sup&gt;Hunyuan, Tencent‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;2&lt;/sup&gt;School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;3&lt;/sup&gt;Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  &lt;br&gt;\\n  &lt;sup&gt;*&lt;/sup&gt;Equal contribution‚ÄÉ\\n  &lt;sup&gt;‚úù&lt;/sup&gt;Corresponding author\\n&lt;/div&gt;\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-16T04:54:58+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>unsloth/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model: baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-10T10:38:04+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td></td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model: baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-10T11:01:33+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td></td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model:\\n- baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-09T01:16:03+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td></td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model:\\n- baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-09T09:40:28+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td></td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model:\\n- baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-09T11:34:57+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td></td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&amp;color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n  &lt;a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&amp;logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n&lt;div align=\"center\" style=\"line-height: 1;\"&gt;\\n  &lt;a href=\"#license\" style=\"margin: 2px;\"&gt;\\n    &lt;img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/&gt;\\n  &lt;/a&gt;\\n&lt;/div&gt;\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n&gt; [!NOTE]\\n&gt; Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n&gt; [!NOTE]\\n&gt; To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        source_node  \\\n",
       "0                      tencent/SRPO   \n",
       "1                      tencent/SRPO   \n",
       "2                      tencent/SRPO   \n",
       "3                      tencent/SRPO   \n",
       "4  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "5  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "6  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "7  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "8  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "9  baidu/ERNIE-4.5-21B-A3B-Thinking   \n",
       "\n",
       "                                       dest_node              edge_type  \\\n",
       "0                      rockerBOO/flux.1-dev-SRPO   model_finetune_model   \n",
       "1           wikeeyang/SRPO-Refine-Quantized-v1.0  model_quantized_model   \n",
       "2                                befox/SRPO-GGUF  model_quantized_model   \n",
       "3                     wikeeyang/SRPO-for-ComfyUI  model_quantized_model   \n",
       "4             unsloth/ERNIE-4.5-21B-A3B-Thinking   model_finetune_model   \n",
       "5        unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF  model_quantized_model   \n",
       "6  gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF  model_quantized_model   \n",
       "7    cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit  model_quantized_model   \n",
       "8    cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit  model_quantized_model   \n",
       "9   mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF  model_quantized_model   \n",
       "\n",
       "   edge_attr source_type dest_type  \\\n",
       "0          0       model     model   \n",
       "1          3       model     model   \n",
       "2          3       model     model   \n",
       "3          3       model     model   \n",
       "4          0       model     model   \n",
       "5          3       model     model   \n",
       "6          3       model     model   \n",
       "7          3       model     model   \n",
       "8          3       model     model   \n",
       "9          3       model     model   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 source_description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Acknowledgement\\n\\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\\n\\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\\n\\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\\n\\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\\n\\n\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n## üîë Inference\\n\\n### Using ComfyUI\\n\\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\\n\\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\\n\\nTip: The workflow JSON info was added to the image file.\\n\\n![Example](comfyui/SRPO-workflow.png)\\n\\n### Quick start\\n```bash\\nfrom diffusers import FluxPipeline\\nfrom safetensors.torch import load_file\\n\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "4  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "5  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "6  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "7  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "8  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "9  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                dest_description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ---\\nbase_model:\\n- tencent/SRPO\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\n---\\n\\n## bf16 and (remaking FP8 version) versions of SRPO from Tencent\\n\\n<div align=\"center\" style=\"font-family: charter;\">\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n</div>\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n\\n## Quick Started\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n\\n#### Inference\\nReplace the `diffusion_pytorch_model.safetensors` of FLUX\\n```python\\nfrom diffusers import FluxPipeline\\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\\npipe = FluxPipeline.from_pretrained('./data/flux',\\n        torch_dtype=torch.bfloat16,\\n        use_safetensors=True\\n    ).to(\"cuda\")\\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\\npipe.transformer.load_state_dict(state_dict)\\nimage = pipe(\\n    prompt,\\n    guidance_scale=3.5,\\n    height=1024,\\n    width=1024,\\n    num_inference_steps=infer_step,\\n    max_sequence_length=512,\\n    generator=generator\\n).images[0]\\n```\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\nlanguage:\\n- en\\nbase_model:\\n- tencent/SRPO\\n---\\n===================================================================================\\n\\nÊú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ Á≤æË∞É Âíå 8bit/4bit (fp8_e4m3fn/Q8_0/Q4_1) ÈáèÂåñÁâàÊú¨Ôºå‰∏ªË¶ÅÊèêÂçáÂá∫ÂõæÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÊ®°ÂûãÁöÑÂÖºÂÆπÊÄß(Á¨¨‰∏ÄÂº†ÂõæÁâá‰∏≠ÁöÑ SRPO-fp8 ÈáèÂåñÁîüÊàêÁöÑÂõæÁâáÔºåÊòæÂæóÁâπÂà´Ê®°Á≥äÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÈááÁî® ComfyUI Ê®°ÂûãÂä†ËΩΩÂπ∂Áõ¥Êé•ÈáèÂåñÁöÑÊñπÂºèÈÄ†ÊàêÔºåÂπ∂ÈùûÊ®°Âûã fp8 Á≤æÂ∫¶‰∏ãÁöÑÂÆûÈôÖË°®Áé∞ÔºåÂÆûÈôÖË°®Áé∞ËØ∑ÂèÇÈòÖÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºå‰∏∫ÈÅøÂÖç‰ΩøÁî®ËÄÖËØØËß£ÔºåÁâπÊèê‰æõÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºåÊ®°ÂûãÂú®‰∏çÂêåÁ≤æÂ∫¶‰∏ãÁöÑË°®Áé∞ÊòØÊ≠£Â∏∏ÁöÑ)„ÄÇ\\n\\nThis model is the refine and quantized version of the model: https://huggingface.co/tencent/SRPO, it improve the clarity of the generated images and the compatibility of the models.\\n(In below image, the SRPO-fp8 means load and quantized directly by ComfyUI diffusion model loader nodes)\\n<p align=\"center\">\\n    <img src=\"Compare.jpg\" width=\"1200\"/>\\n<p>\\n\\n<u>Compare SRPO offical and R&Q v1.0 in the same quantized accuracy:</u>\\n\\n<p align=\"center\">\\n    <img src=\"Compare-02.jpg\" width=\"1200\"/>\\n<p>\\n\\n## Example workflow: Please refer to workflow.png\\n\\n## License Agreement\\n\\nPlease fall under SRPO license refer license.txt file and refer to the FLUX.1 [dev] Non-Commercial License. \\n\\nAlso: https://civitai.com/models/1953067\\n\\n‰ª•‰∏ãÈÉ®ÂàÜÂºïÁî®Ëá™ÂéüÊ®°ÂûãËØ¥ÊòéÂÜÖÂÆπÔºö\\n\\n===================================================================================\\n\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ---\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: tencent-hunyuan-community\\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\\npipeline_tag: text-to-image\\nlanguage:\\n- en\\nbase_model:\\n- tencent/SRPO\\n---\\n===================================================================================\\n\\nÊú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ ËΩ¨Êç¢ Âíå 8bit/4bit (fp8_e4m3fn/Q8_0/Q4_1) ÈáèÂåñÁâàÊú¨Ôºå‰ª•ÈÄÇÈÖç ComfyUI Áî®Êà∑ÁéØÂ¢ÉÊ≠£Â∏∏Âä†ËΩΩÂíåÂá∫ÂõæÔºå‰øùÊåÅÂéüÊ®°ÂûãÊ≠£Â∏∏ÁöÑÂá∫ÂõæÊïàÊûú„ÄÇ\\n\\nThis model is the converted and quantized version of the model: https://huggingface.co/tencent/SRPO, To adapt the ComfyUI environment for normal loading and output of images, maintaining the original model's normal effects.\\n\\n<u> For bf16 version, Pls download it from: https://www.modelscope.cn/models/wikeeyang/SRPO-for-ComfyUI </u>\\n\\n<p align=\"center\">\\n    <img src=\"example.jpg\" width=\"1200\"/>\\n<p>\\n\\n## License Agreement\\n\\nPlease fall under SRPO license refer license.txt file and refer to the FLUX.1 [dev] Non-Commercial License. \\n\\n\\n‰ª•‰∏ãÈÉ®ÂàÜÂºïÁî®Ëá™ÂéüÊ®°ÂûãËØ¥ÊòéÂÜÖÂÆπÔºö\\n\\n===================================================================================\\n\\n\\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\\n<div align=\"center\">\\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \\n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\\n</div>\\n<div align=\"center\">\\n  Xiangwei Shen<sup>1,2*</sup>,\\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \\n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\\n  Yingfang Zhang<sup>1</sup>,\\n  Donghao Li<sup>1</sup>,\\n  <br>\\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\\n</div>\\n<div align=\"center\">\\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\\n  <br>\\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\\n  <br>\\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\\n  <br>\\n  <sup>*</sup>Equal contribution‚ÄÉ\\n  <sup>‚úù</sup>Corresponding author\\n</div>\\n\\n\\n\\n## Abstract\\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\\n### Checkpoints\\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\\n\\n### License\\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\\n## Citation\\nIf you use SRPO for your research, please cite our paper:\\n\\n```bibtex\\n@misc{shen2025directlyaligningdiffusiontrajectory,\\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \\n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\\n      year={2025},\\n      eprint={2509.06942},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI},\\n      url={https://arxiv.org/abs/2509.06942}, \\n}\\n```   \n",
       "4     ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model: baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "5     ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model: baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "6                      ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model:\\n- baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```   \n",
       "7  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model:\\n- baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "8  ---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\npipeline_tag: text-generation\\ntags:\\n- ERNIE4.5\\nlibrary_name: transformers\\nbase_model:\\n- baidu/ERNIE-4.5-21B-A3B-Thinking\\n---\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"#license\" style=\"margin: 2px;\">\\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n# ERNIE-4.5-21B-A3B-Thinking\\n\\n## Model Highlights\\n\\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\\n\\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\\n* **Efficient tool usage** capabilities.\\n* **Enhanced 128K long-context understanding** capabilities.\\n\\n> [!NOTE]\\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\\n\\n![benchmark](./benchmark.png)\\n\\n## Model Overview\\n\\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\\n\\n|Key|Value|\\n|-|-|\\n|Modality|Text|\\n|Training Stage|Posttraining|\\n|Params(Total / Activated)|21B / 3B|\\n|Layers|28|\\n|Heads(Q/KV)|20 / 4|\\n|Text Experts(Total / Activated)|64 / 6|\\n|Vision Experts(Total / Activated)|64 / 6|\\n|Shared Experts|2|\\n|Context Length|131072|\\n\\n## Quickstart\\n\\n> [!NOTE]\\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\\n\\n### FastDeploy Inference\\n\\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\\n\\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\\n\\n```bash\\npython -m fastdeploy.entrypoints.openai.api_server \\\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\\n       --port 8180 \\\\n       --metrics-port 8181 \\\\n       --engine-worker-queue-port 8182 \\\\n       --load_choices \"default_v1\" \\\\n       --tensor-parallel-size 1 \\\\n       --max-model-len 131072 \\\\n       --reasoning-parser ernie_x1 \\\\n       --tool-call-parser ernie_x1 \\\\n       --max-num-seqs 32\\n```\\n\\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\\n\\n```bash\\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\\n-H \"Content-Type: application/json\" \\\\n-d $'{\\n  \"messages\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"How \\'s the weather in Beijing today?\"\\n    }\\n  ],\\n  \"tools\": [\\n    {\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"get_weather\",\\n        \"description\": \"Determine weather in my location\",\\n        \"parameters\": {\\n          \"type\": \"object\",\\n          \"properties\": {\\n            \"location\": {\\n              \"type\": \"string\",\\n              \"description\": \"The city and state e.g. San Francisco, CA\"\\n            },\\n            \"unit\": {\\n              \"type\": \"string\",\\n              \"enum\": [\\n                \"c\",\\n                \"f\"\\n              ]\\n            }\\n          },\\n          \"additionalProperties\": false,\\n          \"required\": [\\n            \"location\",\\n            \"unit\"\\n          ]\\n        },\\n        \"strict\": true\\n      }\\n    }]\\n}'\\n```\\n\\n### vLLM inference\\n\\n```bash\\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\\n```\\n\\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie are currently under development.\\n\\n### Using `transformers` library\\n\\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\\n\\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\\n\\n# load the tokenizer and the model\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n)\\n\\n# prepare the model input\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\\n\\n# conduct text completion\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=1024\\n)\\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\\n\\n# decode the generated ids\\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(\"generate_text:\", generate_text)\\n```\\n\\n## License\\n\\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\\n\\n## Citation\\n\\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\\n\\n```text\\n@misc{ernie2025technicalreport,\\n      title={ERNIE 4.5 Technical Report},\\n      author={Baidu-ERNIE-Team},\\n      year={2025},\\n      primaryClass={cs.CL},\\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\\n}\\n```\\n\\n   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "\n",
       "            source_createdAt source_y_multi_lab  \\\n",
       "0  2025-09-08T12:44:15+00:00               [19]   \n",
       "1  2025-09-08T12:44:15+00:00               [19]   \n",
       "2  2025-09-08T12:44:15+00:00               [19]   \n",
       "3  2025-09-08T12:44:15+00:00               [19]   \n",
       "4  2025-09-08T14:18:31+00:00                [0]   \n",
       "5  2025-09-08T14:18:31+00:00                [0]   \n",
       "6  2025-09-08T14:18:31+00:00                [0]   \n",
       "7  2025-09-08T14:18:31+00:00                [0]   \n",
       "8  2025-09-08T14:18:31+00:00                [0]   \n",
       "9  2025-09-08T14:18:31+00:00                [0]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                        source_relationships  \\\n",
       "0                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "1                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "2                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "3                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "4  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "5  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "6  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "7  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "8  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "9  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "\n",
       "                                                                                                                                                             source_y  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "7  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "              dest_createdAt dest_y_multi_lab  \\\n",
       "0  2025-09-10T21:11:28+00:00             [19]   \n",
       "1  2025-09-13T05:29:39+00:00             [19]   \n",
       "2                        NaN              NaN   \n",
       "3  2025-09-16T04:54:58+00:00             [19]   \n",
       "4  2025-09-10T10:38:04+00:00              [0]   \n",
       "5  2025-09-10T11:01:33+00:00              [0]   \n",
       "6  2025-09-09T01:16:03+00:00              [0]   \n",
       "7  2025-09-09T09:40:28+00:00              [0]   \n",
       "8  2025-09-09T11:34:57+00:00              [0]   \n",
       "9                        NaN              NaN   \n",
       "\n",
       "                                       dest_relationships  \\\n",
       "0  model_finetune_model:Alissonerdx/flux.1-dev-SRPO-LoRas   \n",
       "1                                                           \n",
       "2                                                     NaN   \n",
       "3                                                           \n",
       "4                                                           \n",
       "5                                                           \n",
       "6                                                           \n",
       "7                                                           \n",
       "8                                                           \n",
       "9                                                     NaN   \n",
       "\n",
       "                                                                                                                                                               dest_y  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2                                                                                                                                                                 NaN  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "5  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "6  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "7  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "8  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "9                                                                                                                                                                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: create indexed copies of nodes_df for joining ---\n",
    "nodes_src = nodes_df.set_index([\"id\", \"type\"]).add_prefix(\"source_\")\n",
    "nodes_dst = nodes_df.set_index([\"id\", \"type\"]).add_prefix(\"dest_\")\n",
    "\n",
    "# --- Step 2: join on (source_node, source_type) ---\n",
    "a = edges_df.join(\n",
    "    nodes_src,\n",
    "    on=[\"source_node\", \"source_type\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Step 3: join on (dest_node, dest_type) ---\n",
    "edges_full_left_join = a.join(\n",
    "    nodes_dst,\n",
    "    on=[\"dest_node\", \"dest_type\"],\n",
    "    how=\"left\",\n",
    "    lsuffix=\"_src\", rsuffix=\"_dst\"\n",
    ")\n",
    "\n",
    "# --- Step 4: select & rename columns cleanly ---\n",
    "edges_full_left_join = edges_full_left_join[\n",
    "    [\n",
    "        \"source_node\", \"dest_node\", \"edge_type\", \"edge_attr\", \"source_type\", \"dest_type\",\n",
    "        \"source_description\", \"dest_description\",\n",
    "        \"source_createdAt\", \"source_y_multi_lab\", \"source_relationships\", \"source_y\",\n",
    "        \"dest_createdAt\", \"dest_y_multi_lab\", \"dest_relationships\", \"dest_y\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Combined dataframe shape: {edges_full_left_join.shape}\")\n",
    "display(edges_full_left_join.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad56780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning descriptions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299702/299702 [04:26<00:00, 1124.18it/s]\n",
      "üßπ Cleaning descriptions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299702/299702 [01:44<00:00, 2863.25it/s] \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Step 1. Helper: clean and lighten long descriptions ----------\n",
    "def clean_description(text):\n",
    "    \"\"\"Remove HTML/Markdown/code clutter and extract pipeline_tag if available.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\", None\n",
    "\n",
    "    # Extract pipeline_tag if present\n",
    "    pipeline_tag_match = re.search(r\"pipeline_tag:\\s*([^\\n]+)\", text)\n",
    "    pipeline_tag = pipeline_tag_match.group(1).strip() if pipeline_tag_match else None\n",
    "\n",
    "    # Remove code blocks, HTML, markdown, and citations\n",
    "    cleaned = re.sub(r\"```.*?```\", \" \", text, flags=re.DOTALL)           # code blocks\n",
    "    cleaned = re.sub(r\"<[^>]+>\", \" \", cleaned)                           # HTML tags\n",
    "    cleaned = re.sub(r\"!\\[[^\\]]*\\]\\([^)]+\\)\", \" \", cleaned)              # images\n",
    "    cleaned = re.sub(r\"\\[[^\\]]*\\]\\([^)]+\\)\", \" \", cleaned)               # links\n",
    "    cleaned = re.sub(r\"#+\\s*\", \" \", cleaned)                             # markdown headers\n",
    "    cleaned = re.sub(r\"---+\", \" \", cleaned)                              # separators\n",
    "    cleaned = re.sub(r\"[\\*_`>]+\", \" \", cleaned)                          # markdown symbols\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()                       # extra spaces\n",
    "\n",
    "    # Optionally shorten extremely long text\n",
    "    if len(cleaned) > 800:\n",
    "        cleaned = cleaned[:800] + \" ...\"\n",
    "\n",
    "    return cleaned, pipeline_tag\n",
    "\n",
    "\n",
    "# ---------- Step 2. Apply cleaning to both source and destination ----------\n",
    "def preprocess_descriptions(df):\n",
    "    \"\"\"Apply description cleaning and extract pipeline tags for both source/dest.\"\"\"\n",
    "    tqdm.pandas(desc=\"üßπ Cleaning descriptions\")\n",
    "\n",
    "    df[[\"source_description\", \"source_pipeline_tag\"]] = df[\"source_description\"].progress_apply(\n",
    "        lambda x: pd.Series(clean_description(x))\n",
    "    )\n",
    "\n",
    "    df[[\"dest_description\", \"dest_pipeline_tag\"]] = df[\"dest_description\"].progress_apply(\n",
    "        lambda x: pd.Series(clean_description(x))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "edges_full_left_join = preprocess_descriptions(edges_full_left_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6213412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_node</th>\n",
       "      <th>dest_node</th>\n",
       "      <th>edge_type</th>\n",
       "      <th>edge_attr</th>\n",
       "      <th>source_type</th>\n",
       "      <th>dest_type</th>\n",
       "      <th>source_description</th>\n",
       "      <th>dest_description</th>\n",
       "      <th>source_createdAt</th>\n",
       "      <th>source_y_multi_lab</th>\n",
       "      <th>source_relationships</th>\n",
       "      <th>source_y</th>\n",
       "      <th>dest_createdAt</th>\n",
       "      <th>dest_y_multi_lab</th>\n",
       "      <th>dest_relationships</th>\n",
       "      <th>dest_y</th>\n",
       "      <th>source_pipeline_tag</th>\n",
       "      <th>dest_pipeline_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>rockerBOO/flux.1-dev-SRPO</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &amp;nbsp; &amp;nbsp; &amp;nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...</td>\n",
       "      <td>base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &amp;nbsp; &amp;nbsp; &amp;nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-10T21:11:28+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:Alissonerdx/flux.1-dev-SRPO-LoRas</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>text-to-image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-Refine-Quantized-v1.0</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &amp;nbsp; &amp;nbsp; &amp;nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...</td>\n",
       "      <td>library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image language: - en base model: - tencent/SRPO =================================================================================== Êú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ Á≤æË∞É Âíå 8bit/4bit (fp8 e4m3fn/Q8 0/Q4 1) ÈáèÂåñÁâàÊú¨Ôºå‰∏ªË¶ÅÊèêÂçáÂá∫ÂõæÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÊ®°ÂûãÁöÑÂÖºÂÆπÊÄß(Á¨¨‰∏ÄÂº†ÂõæÁâá‰∏≠ÁöÑ SRPO-fp8 ÈáèÂåñÁîüÊàêÁöÑÂõæÁâáÔºåÊòæÂæóÁâπÂà´Ê®°Á≥äÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÈááÁî® ComfyUI Ê®°ÂûãÂä†ËΩΩÂπ∂Áõ¥Êé•ÈáèÂåñÁöÑÊñπÂºèÈÄ†ÊàêÔºåÂπ∂ÈùûÊ®°Âûã fp8 Á≤æÂ∫¶‰∏ãÁöÑÂÆûÈôÖË°®Áé∞ÔºåÂÆûÈôÖË°®Áé∞ËØ∑ÂèÇÈòÖÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºå‰∏∫ÈÅøÂÖç‰ΩøÁî®ËÄÖËØØËß£ÔºåÁâπÊèê‰æõÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºåÊ®°ÂûãÂú®‰∏çÂêåÁ≤æÂ∫¶‰∏ãÁöÑË°®Áé∞ÊòØÊ≠£Â∏∏ÁöÑ)„ÄÇ This model is the refine and quantized version of the model: https://huggingface.co/tencent/SRPO, it improve the clarity of the generated images and the compatibility of the models. (In below image, the SRPO-fp8 means load and quantized directly by Comf ...</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-13T05:29:39+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>text-to-image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>befox/SRPO-GGUF</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &amp;nbsp; &amp;nbsp; &amp;nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...</td>\n",
       "      <td></td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tencent/SRPO</td>\n",
       "      <td>wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>model_quantized_model</td>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &amp;nbsp; &amp;nbsp; &amp;nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...</td>\n",
       "      <td>library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image language: - en base model: - tencent/SRPO =================================================================================== Êú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ ËΩ¨Êç¢ Âíå 8bit/4bit (fp8 e4m3fn/Q8 0/Q4 1) ÈáèÂåñÁâàÊú¨Ôºå‰ª•ÈÄÇÈÖç ComfyUI Áî®Êà∑ÁéØÂ¢ÉÊ≠£Â∏∏Âä†ËΩΩÂíåÂá∫ÂõæÔºå‰øùÊåÅÂéüÊ®°ÂûãÊ≠£Â∏∏ÁöÑÂá∫ÂõæÊïàÊûú„ÄÇ This model is the converted and quantized version of the model: https://huggingface.co/tencent/SRPO, To adapt the ComfyUI environment for normal loading and output of images, maintaining the original model's normal effects. For bf16 version, Pls download it from: https://www.modelscope.cn/models/wikeeyang/SRPO-for-ComfyUI License Agreement Please fall under SRPO  ...</td>\n",
       "      <td>2025-09-08T12:44:15+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td>model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-16T04:54:58+00:00</td>\n",
       "      <td>[19]</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>text-to-image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baidu/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>unsloth/ERNIE-4.5-21B-A3B-Thinking</td>\n",
       "      <td>model_finetune_model</td>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>license: apache-2.0 language: - en - zh pipeline tag: text-generation tags: - ERNIE4.5 library name: transformers ERNIE-4.5-21B-A3B-Thinking Model Highlights Over the past three months, we have continued to scale the thinking capability of ERNIE-4.5-21B-A3B, improving both the quality and depth of reasoning, thereby advancing the competitiveness of ERNIE lightweight models in complex reasoning tasks. We are pleased to introduce ERNIE-4.5-21B-A3B-Thinking , featuring the following key enhancements: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise. Efficient tool usage capabilities. Enhanced 128K long-context understanding capabilities. [!NOTE] Note ...</td>\n",
       "      <td>license: apache-2.0 language: - en - zh pipeline tag: text-generation tags: - ERNIE4.5 library name: transformers base model: baidu/ERNIE-4.5-21B-A3B-Thinking ERNIE-4.5-21B-A3B-Thinking Model Highlights Over the past three months, we have continued to scale the thinking capability of ERNIE-4.5-21B-A3B, improving both the quality and depth of reasoning, thereby advancing the competitiveness of ERNIE lightweight models in complex reasoning tasks. We are pleased to introduce ERNIE-4.5-21B-A3B-Thinking , featuring the following key enhancements: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise. Efficient tool usage capabilities. Enhanced 128K long-con ...</td>\n",
       "      <td>2025-09-08T14:18:31+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td>model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2025-09-10T10:38:04+00:00</td>\n",
       "      <td>[0]</td>\n",
       "      <td></td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>text-generation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        source_node                             dest_node  \\\n",
       "0                      tencent/SRPO             rockerBOO/flux.1-dev-SRPO   \n",
       "1                      tencent/SRPO  wikeeyang/SRPO-Refine-Quantized-v1.0   \n",
       "2                      tencent/SRPO                       befox/SRPO-GGUF   \n",
       "3                      tencent/SRPO            wikeeyang/SRPO-for-ComfyUI   \n",
       "4  baidu/ERNIE-4.5-21B-A3B-Thinking    unsloth/ERNIE-4.5-21B-A3B-Thinking   \n",
       "\n",
       "               edge_type  edge_attr source_type dest_type  \\\n",
       "0   model_finetune_model          0       model     model   \n",
       "1  model_quantized_model          3       model     model   \n",
       "2  model_quantized_model          3       model     model   \n",
       "3  model_quantized_model          3       model     model   \n",
       "4   model_finetune_model          0       model     model   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     source_description  \\\n",
       "0  library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...   \n",
       "1  library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...   \n",
       "2  library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...   \n",
       "3  library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...   \n",
       "4  license: apache-2.0 language: - en - zh pipeline tag: text-generation tags: - ERNIE4.5 library name: transformers ERNIE-4.5-21B-A3B-Thinking Model Highlights Over the past three months, we have continued to scale the thinking capability of ERNIE-4.5-21B-A3B, improving both the quality and depth of reasoning, thereby advancing the competitiveness of ERNIE lightweight models in complex reasoning tasks. We are pleased to introduce ERNIE-4.5-21B-A3B-Thinking , featuring the following key enhancements: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise. Efficient tool usage capabilities. Enhanced 128K long-context understanding capabilities. [!NOTE] Note ...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       dest_description  \\\n",
       "0  base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...   \n",
       "1  library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image language: - en base model: - tencent/SRPO =================================================================================== Êú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ Á≤æË∞É Âíå 8bit/4bit (fp8 e4m3fn/Q8 0/Q4 1) ÈáèÂåñÁâàÊú¨Ôºå‰∏ªË¶ÅÊèêÂçáÂá∫ÂõæÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÊ®°ÂûãÁöÑÂÖºÂÆπÊÄß(Á¨¨‰∏ÄÂº†ÂõæÁâá‰∏≠ÁöÑ SRPO-fp8 ÈáèÂåñÁîüÊàêÁöÑÂõæÁâáÔºåÊòæÂæóÁâπÂà´Ê®°Á≥äÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÈááÁî® ComfyUI Ê®°ÂûãÂä†ËΩΩÂπ∂Áõ¥Êé•ÈáèÂåñÁöÑÊñπÂºèÈÄ†ÊàêÔºåÂπ∂ÈùûÊ®°Âûã fp8 Á≤æÂ∫¶‰∏ãÁöÑÂÆûÈôÖË°®Áé∞ÔºåÂÆûÈôÖË°®Áé∞ËØ∑ÂèÇÈòÖÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºå‰∏∫ÈÅøÂÖç‰ΩøÁî®ËÄÖËØØËß£ÔºåÁâπÊèê‰æõÁ¨¨‰∫åÂº†ÂØπÊØîÂõæÔºåÊ®°ÂûãÂú®‰∏çÂêåÁ≤æÂ∫¶‰∏ãÁöÑË°®Áé∞ÊòØÊ≠£Â∏∏ÁöÑ)„ÄÇ This model is the refine and quantized version of the model: https://huggingface.co/tencent/SRPO, it improve the clarity of the generated images and the compatibility of the models. (In below image, the SRPO-fp8 means load and quantized directly by Comf ...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "3  library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image language: - en base model: - tencent/SRPO =================================================================================== Êú¨Ê®°Âûã‰∏∫ https://huggingface.co/tencent/SRPO Ê®°ÂûãÁöÑ ËΩ¨Êç¢ Âíå 8bit/4bit (fp8 e4m3fn/Q8 0/Q4 1) ÈáèÂåñÁâàÊú¨Ôºå‰ª•ÈÄÇÈÖç ComfyUI Áî®Êà∑ÁéØÂ¢ÉÊ≠£Â∏∏Âä†ËΩΩÂíåÂá∫ÂõæÔºå‰øùÊåÅÂéüÊ®°ÂûãÊ≠£Â∏∏ÁöÑÂá∫ÂõæÊïàÊûú„ÄÇ This model is the converted and quantized version of the model: https://huggingface.co/tencent/SRPO, To adapt the ComfyUI environment for normal loading and output of images, maintaining the original model's normal effects. For bf16 version, Pls download it from: https://www.modelscope.cn/models/wikeeyang/SRPO-for-ComfyUI License Agreement Please fall under SRPO  ...   \n",
       "4  license: apache-2.0 language: - en - zh pipeline tag: text-generation tags: - ERNIE4.5 library name: transformers base model: baidu/ERNIE-4.5-21B-A3B-Thinking ERNIE-4.5-21B-A3B-Thinking Model Highlights Over the past three months, we have continued to scale the thinking capability of ERNIE-4.5-21B-A3B, improving both the quality and depth of reasoning, thereby advancing the competitiveness of ERNIE lightweight models in complex reasoning tasks. We are pleased to introduce ERNIE-4.5-21B-A3B-Thinking , featuring the following key enhancements: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise. Efficient tool usage capabilities. Enhanced 128K long-con ...   \n",
       "\n",
       "            source_createdAt source_y_multi_lab  \\\n",
       "0  2025-09-08T12:44:15+00:00               [19]   \n",
       "1  2025-09-08T12:44:15+00:00               [19]   \n",
       "2  2025-09-08T12:44:15+00:00               [19]   \n",
       "3  2025-09-08T12:44:15+00:00               [19]   \n",
       "4  2025-09-08T14:18:31+00:00                [0]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                        source_relationships  \\\n",
       "0                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "1                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "2                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "3                                                                                                                                                                                                                                                    model_finetune_model:rockerBOO/flux.1-dev-SRPO, model_quantized_model:wikeeyang/SRPO-Refine-Quantized-v1.0, befox/SRPO-GGUF, wikeeyang/SRPO-for-ComfyUI   \n",
       "4  model_finetune_model:unsloth/ERNIE-4.5-21B-A3B-Thinking, model_quantized_model:unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF, gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit, cpatonn/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit, mradermacher/ERNIE-4.5-21B-A3B-Thinking-GGUF, nightmedia/ERNIE-4.5-21B-A3B-Thinking-mxfp4-mlx, wekW/ERNIE-4.5-21B-A3B-Thinking-Q8_0-GGUF   \n",
       "\n",
       "                                                                                                                                                             source_y  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "              dest_createdAt dest_y_multi_lab  \\\n",
       "0  2025-09-10T21:11:28+00:00             [19]   \n",
       "1  2025-09-13T05:29:39+00:00             [19]   \n",
       "2                        NaN              NaN   \n",
       "3  2025-09-16T04:54:58+00:00             [19]   \n",
       "4  2025-09-10T10:38:04+00:00              [0]   \n",
       "\n",
       "                                       dest_relationships  \\\n",
       "0  model_finetune_model:Alissonerdx/flux.1-dev-SRPO-LoRas   \n",
       "1                                                           \n",
       "2                                                     NaN   \n",
       "3                                                           \n",
       "4                                                           \n",
       "\n",
       "                                                                                                                                                               dest_y  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2                                                                                                                                                                 NaN   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "  source_pipeline_tag dest_pipeline_tag  \n",
       "0       text-to-image     text-to-image  \n",
       "1       text-to-image     text-to-image  \n",
       "2       text-to-image              None  \n",
       "3       text-to-image     text-to-image  \n",
       "4     text-generation   text-generation  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_full_left_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eab66be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Parquet failed, saved as Pickle instead:\n",
      "/home/hice1/cxu371/scratch/final_edges_full_left_join.pkl\n",
      "Error: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "üì¶ File size: 438.25 MB\n",
      "‚úÖ RAM after cleanup: 5.89 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gc, torch, psutil\n",
    "\n",
    "save_path = \"/home/hice1/cxu371/scratch/final_edges_full_left_join.parquet\"\n",
    "\n",
    "# --- Try Parquet first (fast, compact) ---\n",
    "try:\n",
    "    edges_full_left_join.to_parquet(save_path, index=False)\n",
    "    print(f\"‚úÖ Saved cleaned edges_full_left_join to Parquet:\\n{save_path}\")\n",
    "except Exception as e:\n",
    "    # Fallback to Pickle if Parquet libraries aren't available\n",
    "    alt_path = save_path.replace(\".parquet\", \".pkl\")\n",
    "    edges_full_left_join.to_pickle(alt_path)\n",
    "    save_path = alt_path\n",
    "    print(f\"‚ö†Ô∏è Parquet failed, saved as Pickle instead:\\n{save_path}\\nError: {e}\")\n",
    "\n",
    "# --- Check file size ---\n",
    "if os.path.exists(save_path):\n",
    "    size_mb = os.path.getsize(save_path) / 1e6\n",
    "    print(f\"üì¶ File size: {size_mb:.2f} MB\")\n",
    "\n",
    "# --- Optional: clear up memory (keeps encoder intact) ---\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"‚úÖ RAM after cleanup: {psutil.Process().memory_info().rss / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "69c7f0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>type</th>\n",
       "      <th>total_edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen/Qwen1.5-0.5B</td>\n",
       "      <td>model</td>\n",
       "      <td>30753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen/Qwen1.5-1.8B</td>\n",
       "      <td>model</td>\n",
       "      <td>29059.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/gemma-2b</td>\n",
       "      <td>model</td>\n",
       "      <td>22578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google/gemma-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>8929.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distilbert/distilbert-base-uncased</td>\n",
       "      <td>model</td>\n",
       "      <td>6171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen/Qwen1.5-7B</td>\n",
       "      <td>model</td>\n",
       "      <td>6118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stabilityai/stable-diffusion-xl-base-1.0</td>\n",
       "      <td>model</td>\n",
       "      <td>5516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>black-forest-labs/FLUX.1-dev</td>\n",
       "      <td>model</td>\n",
       "      <td>5192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>model</td>\n",
       "      <td>2945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>openai-community/gpt2</td>\n",
       "      <td>model</td>\n",
       "      <td>2677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FacebookAI/xlm-roberta-base</td>\n",
       "      <td>model</td>\n",
       "      <td>2131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>model</td>\n",
       "      <td>2025.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>openai/whisper-small</td>\n",
       "      <td>model</td>\n",
       "      <td>1770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google-bert/bert-base-uncased</td>\n",
       "      <td>model</td>\n",
       "      <td>1770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>google-bert/bert-base-cased</td>\n",
       "      <td>model</td>\n",
       "      <td>1686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google/vit-base-patch16-224-in21k</td>\n",
       "      <td>model</td>\n",
       "      <td>1648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>model</td>\n",
       "      <td>1623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Qwen/Qwen2-1.5B</td>\n",
       "      <td>model</td>\n",
       "      <td>1444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google-t5/t5-small</td>\n",
       "      <td>model</td>\n",
       "      <td>1426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>model</td>\n",
       "      <td>1385.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        node   type  total_edges\n",
       "0                          Qwen/Qwen1.5-0.5B  model      30753.0\n",
       "1                          Qwen/Qwen1.5-1.8B  model      29059.0\n",
       "2                            google/gemma-2b  model      22578.0\n",
       "3                            google/gemma-7b  model       8929.0\n",
       "4         distilbert/distilbert-base-uncased  model       6171.0\n",
       "5                            Qwen/Qwen1.5-7B  model       6118.0\n",
       "6   stabilityai/stable-diffusion-xl-base-1.0  model       5516.0\n",
       "7               black-forest-labs/FLUX.1-dev  model       5192.0\n",
       "8                unsloth/llama-3-8b-bnb-4bit  model       2945.0\n",
       "9                      openai-community/gpt2  model       2677.0\n",
       "10               FacebookAI/xlm-roberta-base  model       2131.0\n",
       "11                 mistralai/Mistral-7B-v0.1  model       2025.0\n",
       "12                      openai/whisper-small  model       1770.0\n",
       "13             google-bert/bert-base-uncased  model       1770.0\n",
       "14               google-bert/bert-base-cased  model       1686.0\n",
       "15         google/vit-base-patch16-224-in21k  model       1648.0\n",
       "16                  meta-llama/Llama-2-7b-hf  model       1623.0\n",
       "17                           Qwen/Qwen2-1.5B  model       1444.0\n",
       "18                        google-t5/t5-small  model       1426.0\n",
       "19        mistralai/Mistral-7B-Instruct-v0.2  model       1385.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count appearances as source\n",
    "src_counts = (\n",
    "    edges_df.groupby([\"source_node\", \"source_type\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"source_edge_count\")\n",
    ")\n",
    "\n",
    "# Count appearances as destination\n",
    "dst_counts = (\n",
    "    edges_df.groupby([\"dest_node\", \"dest_type\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"dest_edge_count\")\n",
    ")\n",
    "\n",
    "# Merge both counts, filling missing with 0\n",
    "node_counts = pd.merge(\n",
    "    src_counts,\n",
    "    dst_counts,\n",
    "    left_on=[\"source_node\", \"source_type\"],\n",
    "    right_on=[\"dest_node\", \"dest_type\"],\n",
    "    how=\"outer\"\n",
    ").fillna(0)\n",
    "\n",
    "# Consolidate to unified columns\n",
    "node_counts[\"node\"] = node_counts[\"source_node\"].combine_first(node_counts[\"dest_node\"])\n",
    "node_counts[\"type\"] = node_counts[\"source_type\"].combine_first(node_counts[\"dest_type\"])\n",
    "\n",
    "# Total number of edges (as source + as dest)\n",
    "node_counts[\"total_edges\"] = node_counts[\"source_edge_count\"] + node_counts[\"dest_edge_count\"]\n",
    "\n",
    "# Keep only relevant columns\n",
    "node_counts = node_counts[[\"node\", \"type\", \"total_edges\"]].sort_values(\n",
    "    by=\"total_edges\", ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "display(node_counts.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e173936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Edge count percentiles for (node, type) pairs:\n",
      "25% percentile: 1.0\n",
      "50% percentile: 1.0\n",
      "75% percentile: 1.0\n",
      "90% percentile: 2.0\n",
      "99% percentile: 10.0\n",
      "99% percentile: 93.0\n",
      "100% percentile: 30753.0\n"
     ]
    }
   ],
   "source": [
    "percentiles = node_counts[\"total_edges\"].quantile([0.25, 0.5, 0.75, 0.9, 0.99, 0.999, 1])\n",
    "print(\"üìà Edge count percentiles for (node, type) pairs:\")\n",
    "for p, v in percentiles.items():\n",
    "    print(f\"{int(p*100)}% percentile: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b387f53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Building triples for 299,702 edges...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating triples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299702/299702 [00:18<00:00, 15831.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Triples generated successfully! Total count: 299,702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 3. Build node text (without relationships) ----------\n",
    "def build_node_text(name, node_type, desc, pipeline_tag):\n",
    "    \"\"\"Build node representation text for triple construction.\"\"\"\n",
    "    parts = []\n",
    "    name = str(name).strip() if isinstance(name, str) else \"\"\n",
    "    node_type = str(node_type).strip() if isinstance(node_type, str) else \"\"\n",
    "    desc = str(desc).strip() if isinstance(desc, str) else \"\"\n",
    "    pipeline_tag = str(pipeline_tag).strip() if isinstance(pipeline_tag, str) else \"\"\n",
    "\n",
    "    if name:\n",
    "        parts.append(f\"Node: {name}\")\n",
    "    if node_type:\n",
    "        parts.append(f\"Type: {node_type}\")\n",
    "    if pipeline_tag:\n",
    "        parts.append(f\"Pipeline: {pipeline_tag}\")\n",
    "    if desc:\n",
    "        parts.append(f\"Description: {desc}\")\n",
    "\n",
    "    return \". \".join(parts)\n",
    "\n",
    "\n",
    "# ---------- Step 4. Build triples ----------\n",
    "triples = []\n",
    "total = len(edges_full_left_join)\n",
    "print(f\"\\nüîó Building triples for {total:,} edges...\\n\")\n",
    "\n",
    "for idx, row in enumerate(\n",
    "    tqdm(edges_full_left_join.itertuples(index=False), total=total, desc=\"Generating triples\")\n",
    "):\n",
    "    src_text = build_node_text(row.source_node, row.source_type, row.source_description, row.source_pipeline_tag)\n",
    "    dst_text = build_node_text(row.dest_node, row.dest_type, row.dest_description, row.dest_pipeline_tag)\n",
    "    relation = str(row.edge_type).strip()\n",
    "    triples.append((src_text, relation, dst_text))\n",
    "\n",
    "    if idx % 5000 == 0:\n",
    "        gc.collect()  # optional: free memory periodically\n",
    "\n",
    "print(f\"‚úÖ Triples generated successfully! Total count: {len(triples):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbd7cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved triples to 'triples_cleaned.pt'\n",
      "Example triple:\n",
      "('Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...', 'model_finetune_model', 'Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...')\n"
     ]
    }
   ],
   "source": [
    "torch.save(triples, \"triples_cleaned.pt\")\n",
    "print(\"üíæ Saved triples to 'triples_cleaned.pt'\")\n",
    "print(\"Example triple:\")\n",
    "print(triples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c9fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = torch.load(\"triples_cleaned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61d01ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the graph data from raw triples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding 278062 strings w/ SentenceTransformer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1087/1087 [26:32<00:00,  1.47s/it]\n",
      "Encoding 5 strings w/ SentenceTransformer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# turn triples into graph\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# creating the embedding model\n",
    "sent_trans_batch_size = 256\n",
    "\n",
    "\n",
    "print(\"Creating the graph data from raw triples...\")\n",
    "# create the graph data from raw triples\n",
    "graph_data = create_graph_from_triples(\n",
    "    triples=triples, embedding_model=encoder.encode,\n",
    "    embedding_method_kwargs={\n",
    "        \"batch_size\": min(len(triples), sent_trans_batch_size),\n",
    "        \"verbose\": True\n",
    "    }, pre_transform=preprocess_triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e387864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved graph_data to 'final_graph_krystal.pt'\n"
     ]
    }
   ],
   "source": [
    "torch.save(graph_data, \"final_graph_krystal.pt\")\n",
    "print(\"üíæ Saved graph_data to 'final_graph_krystal.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "150d58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = torch.load(\"final_graph_krystal.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa229d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[278062, 768], edge_index=[2, 299702], edge_attr=[299702, 768], edge_id=[299702], node_id=[278062])\n"
     ]
    }
   ],
   "source": [
    "print(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56956ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAM after cleanup: 9.54 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"‚úÖ RAM after cleanup: {psutil.Process().memory_info().rss / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc6f9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the graph and feature stores\n",
    "fs, gs = create_remote_backend_from_graph_data(\n",
    "    graph_data=graph_data, path=\"backend\",\n",
    "    graph_db=NeighborSamplingRAGGraphStore,\n",
    "    feature_db=KNNRAGFeatureStore).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3f2fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_filter = make_pcst_filter(\n",
    "    triples,\n",
    "    encoder,\n",
    "    topk=12,         # modest node selection\n",
    "    topk_e=35,       # allow moderate edge budget\n",
    "    cost_e=0.45,     # make edges moderately costly\n",
    "    num_clusters=5   # allow fragmentation for superhubs\n",
    ")\n",
    "\n",
    "fanout = 80         # max neighbors per hop\n",
    "num_hops = 2        # keep expansion shallow\n",
    "\n",
    "query_loader_config = {\n",
    "    \"k_nodes\": 512,  # reduced max KNN retrieval per query\n",
    "    \"num_neighbors\": [fanout] * num_hops,\n",
    "    \"encoder_model\": encoder,\n",
    "}\n",
    "\n",
    "query_loader = RAGQueryLoader(\n",
    "    graph_data=(fs, gs),\n",
    "    subgraph_filter=subgraph_filter,\n",
    "    config=query_loader_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77b23fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of hops is greater than 1, resulting in memory-expensive recursive calls.\n"
     ]
    }
   ],
   "source": [
    "subgraph = query_loader.query('what are some of the tasks for model tencent/SRPO?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84435fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2, 768], edge_index=[2, 6], edge_attr=[6, 768], node_idx=[2], edge_idx=[6], desc='node_id,node_attr\n",
       "0,\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\"\n",
       "1,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\n",
       "src,edge_attr,dst\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "', triples=[6], question='what are some of the tasks for model tencent/SRPO?')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb50b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Total edges in subgraph: 6\n",
      "‚ö†Ô∏è Duplicate edges found: 6\n",
      "‚úÖ Unique edges: 0\n",
      "\n",
      "Examples of duplicated edges:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>edge_hash</th>\n",
       "      <th>dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-37623898268095519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-37623898268095519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-37623898268095519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-37623898268095519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-37623898268095519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   src          edge_hash  dst\n",
       "0    0 -37623898268095519    1\n",
       "1    0 -37623898268095519    1\n",
       "2    0 -37623898268095519    1\n",
       "3    0 -37623898268095519    1\n",
       "4    0 -37623898268095519    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAM after cleanup: 176.11 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# --- Step 1. Extract edge info safely ---\n",
    "src_s = subgraph.edge_index[0].cpu().numpy()\n",
    "dst_s = subgraph.edge_index[1].cpu().numpy()\n",
    "edge_attr_hash_s = [hash(tuple(e.cpu().tolist())) for e in subgraph.edge_attr]\n",
    "\n",
    "subgraph_edges_tmp = pd.DataFrame({\n",
    "    \"src\": src_s,\n",
    "    \"edge_hash\": edge_attr_hash_s,\n",
    "    \"dst\": dst_s\n",
    "})\n",
    "\n",
    "# --- Step 2. Detect duplicates ---\n",
    "dupe_mask_s = subgraph_edges_tmp.duplicated(subset=[\"src\", \"edge_hash\", \"dst\"], keep=False)\n",
    "num_dupes = dupe_mask_s.sum()\n",
    "total_edges = len(subgraph_edges_tmp)\n",
    "\n",
    "print(f\"üîç Total edges in subgraph: {total_edges:,}\")\n",
    "print(f\"‚ö†Ô∏è Duplicate edges found: {num_dupes:,}\")\n",
    "print(f\"‚úÖ Unique edges: {total_edges - num_dupes:,}\")\n",
    "\n",
    "if num_dupes > 0:\n",
    "    print(\"\\nExamples of duplicated edges:\")\n",
    "    display(subgraph_edges_tmp[dupe_mask_s].head(5))\n",
    "\n",
    "# --- Step 3. Cleanup ---\n",
    "del src_s, dst_s, edge_attr_hash_s, dupe_mask_s, subgraph_edges_tmp\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    print(f\"‚úÖ RAM after cleanup: {psutil.virtual_memory().available / 1e9:.2f} GB\")\n",
    "except ImportError:\n",
    "    print(\"‚úÖ Memory cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f54b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adaptive_query_loader(fs, gs, triples, encoder):\n",
    "#     \"\"\"Create a query_loader with settings that scale with graph density.\"\"\"\n",
    "#     num_edges = len(triples)\n",
    "#     num_nodes = len(set([t[0] for t in triples] + [t[2] for t in triples]))\n",
    "#     edge_density = num_edges / max(num_nodes, 1)\n",
    "\n",
    "#     # --- Adaptive thresholds ---\n",
    "#     if edge_density < 2:  # very sparse\n",
    "#         topk, topk_e, cost_e, num_clusters = 10, 25, 0.5, 6\n",
    "#         fanout, num_hops, k_nodes = 60, 2, 512\n",
    "#     elif edge_density < 5:  # medium\n",
    "#         topk, topk_e, cost_e, num_clusters = 15, 40, 0.4, 4\n",
    "#         fanout, num_hops, k_nodes = 120, 3, 768\n",
    "#     else:  # dense graph\n",
    "#         topk, topk_e, cost_e, num_clusters = 25, 60, 0.25, 2\n",
    "#         fanout, num_hops, k_nodes = 200, 3, 1024\n",
    "\n",
    "#     print(f\"üìä Graph density = {edge_density:.2f}\")\n",
    "#     print(f\"‚û°Ô∏è  Using adaptive config: topk={topk}, topk_e={topk_e}, cost_e={cost_e}, \"\n",
    "#           f\"num_clusters={num_clusters}, fanout={fanout}, hops={num_hops}, k_nodes={k_nodes}\")\n",
    "\n",
    "#     subgraph_filter = make_pcst_filter(\n",
    "#         triples,\n",
    "#         encoder,\n",
    "#         topk=topk,\n",
    "#         topk_e=topk_e,\n",
    "#         cost_e=cost_e,\n",
    "#         num_clusters=num_clusters\n",
    "#     )\n",
    "\n",
    "#     query_loader_config = {\n",
    "#         \"k_nodes\": k_nodes,\n",
    "#         \"num_neighbors\": [fanout] * num_hops,\n",
    "#         \"encoder_model\": encoder,\n",
    "#     }\n",
    "\n",
    "#     return RAGQueryLoader(\n",
    "#         graph_data=(fs, gs),\n",
    "#         subgraph_filter=subgraph_filter,\n",
    "#         config=query_loader_config\n",
    "#     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f167ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Graph density = 1.08\n",
      "‚û°Ô∏è  Using adaptive config: topk=10, topk_e=25, cost_e=0.5, num_clusters=6, fanout=60, hops=2, k_nodes=512\n",
      "Warning: Number of hops is greater than 1, resulting in memory-expensive recursive calls.\n"
     ]
    }
   ],
   "source": [
    "# query_loader = adaptive_query_loader(fs, gs, triples, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe574120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of hops is greater than 1, resulting in memory-expensive recursive calls.\n"
     ]
    }
   ],
   "source": [
    "# subgraph = query_loader.query('what are some of the tasks for model tencent/SRPO?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e070f9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2, 768], edge_index=[2, 7], edge_attr=[7, 768], node_idx=[2], edge_idx=[7], desc='node_id,node_attr\n",
       "0,\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\"\n",
       "1,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\n",
       "src,edge_attr,dst\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "\"Node: tencent/SRPO. Type: model. Pipeline: text-to-image. Description: library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, ...\",model_finetune_model,\"Node: rockerBOO/flux.1-dev-SRPO. Type: model. Pipeline: text-to-image. Description: base model: - tencent/SRPO library name: diffusers license: other license name: tencent-hunyuan-community license link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline tag: text-to-image bf16 and (remaking FP8 version) versions of SRPO from Tencent Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference &nbsp; &nbsp; &nbsp; Xiangwei Shen 1,2 , Zhimin Li 1 , Zhantao Yang 1 , Shiyi Zhang 3 , Yingfang Zhang 1 , Donghao Li 1 , Chunyu Wang 1 , Qinglin Lu 1 , Yansong Tang 3,‚úù 1 Hunyuan, Tencent 2 School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3 Shenzhen International Graduate School, Tsinghua University Equal contribution ‚úù Corresponding author Abstract Recent studies have demonstrated the effectiveness of direct ...\"\n",
       "', triples=[7], question='what are some of the tasks for model tencent/SRPO?')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7e307e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subgraph = query_loader.query('Show me all the models that have edges with model Qwen/Qwen1.5-0.5B?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ce8fb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 768], edge_index=[2, 478], edge_attr=[478, 768], node_idx=[3], edge_idx=[478], desc='node_id,node_attr\n",
       "19792,Node: hendra01/qwen-2.5-finetuned. Type: model\n",
       "19793,Node: binh230/qwen2.5-finetuned. Type: model\n",
       "19702,\"Node: Qwen/Qwen2.5-7B-Instruct. Type: model. Pipeline: text-generation. Description: license: apache-2.0 license link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE language: - en pipeline tag: text-generation base model: Qwen/Qwen2.5-7B tags: - chat library name: transformers Qwen2.5-7B-Instruct Introduction Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2: - Significantly more knowledge and has greatly improved capabilities in coding and mathematics , thanks to our specialized expert models in these domains. - Significant improvements in instruction following , generating long texts (over 8K tokens), understanding structured data (e.g, tables), and gene ...\"\n",
       "\n",
       "src,edge_attr,dst\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "Node: FacebookAI/xlm-roberta-base. Type: model. Description: tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - xh - yi - zh license: mit XLM-RoBERTa (base-sized model) XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper by Conneau et al. and first released in . Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model ...,model_finetune_model,\"Node: Urashima/xlm-roberta-base-finetuned-panx-de. Type: model. Description: license: mit base model: xlm-roberta-base tags: - generated from trainer metrics: - f1 model-index: - name: xlm-roberta-base-finetuned-panx-de results: [] xlm-roberta-base-finetuned-panx-de This model is a fine-tuned version of on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.1414 - F1: 0.8568 Model description More information needed Intended uses & limitations More information needed Training and evaluation data More information needed Training procedure Training hyperparameters The following hyperparameters were used during training: - learning rate: 5e-05 - train batch size: 24 - eval batch size: 24 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr scheduler type: linear - num epochs: 3 Training results | Training Loss | ...\"\n",
       "', triples=[478], question='Show me all the models that have edges with model Qwen/Qwen1.5-0.5B?')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "549c5f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Total edges: 299,702\n",
      "‚ö†Ô∏è Duplicate edges found: 0\n",
      "‚úÖ Unique edges: 299,702\n",
      "‚úÖ RAM after cleanup: 176.34 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# --- Step 1. Extract edge data safely ---\n",
    "src = graph_data.edge_index[0].cpu().numpy()\n",
    "dst = graph_data.edge_index[1].cpu().numpy()\n",
    "\n",
    "# Create hashable fingerprints for edge_attr (embeddings)\n",
    "edge_attr_hash = [hash(tuple(e.cpu().tolist())) for e in graph_data.edge_attr]\n",
    "\n",
    "graph_edges_tmp = pd.DataFrame({\n",
    "    \"src\": src,\n",
    "    \"edge_hash\": edge_attr_hash,\n",
    "    \"dst\": dst\n",
    "})\n",
    "\n",
    "# --- Step 2. Detect duplicates ---\n",
    "dupe_mask_tmp = graph_edges_tmp.duplicated(subset=[\"src\", \"edge_hash\", \"dst\"], keep=False)\n",
    "num_dupes = dupe_mask_tmp.sum()\n",
    "total_edges = len(graph_edges_tmp)\n",
    "\n",
    "print(f\"üîç Total edges: {total_edges:,}\")\n",
    "print(f\"‚ö†Ô∏è Duplicate edges found: {num_dupes:,}\")\n",
    "print(f\"‚úÖ Unique edges: {total_edges - num_dupes:,}\")\n",
    "\n",
    "if num_dupes > 0:\n",
    "    print(\"\\nExamples of duplicated edges:\")\n",
    "    display(graph_edges_tmp[dupe_mask_tmp].head(5))\n",
    "\n",
    "# --- Step 3. Cleanup ---\n",
    "del src, dst, edge_attr_hash, dupe_mask_tmp, graph_edges_tmp\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Optional RAM check\n",
    "try:\n",
    "    import psutil\n",
    "    print(f\"‚úÖ RAM after cleanup: {psutil.virtual_memory().available / 1e9:.2f} GB\")\n",
    "except ImportError:\n",
    "    print(\"‚úÖ Memory cleanup complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ac59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlg_fix",
   "language": "python",
   "name": "mlg_fix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
